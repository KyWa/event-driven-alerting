{
    "apiVersion": "v1",
    "items": [
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "annotations": {
                    "capability.openshift.io/name": "NodeTuning",
                    "include.release.openshift.io/hypershift": "true",
                    "include.release.openshift.io/ibm-cloud-managed": "true",
                    "include.release.openshift.io/self-managed-high-availability": "true",
                    "include.release.openshift.io/single-node-developer": "true"
                },
                "creationTimestamp": "2025-12-02T13:23:18Z",
                "generation": 1,
                "labels": {
                    "role": "alert-rules"
                },
                "name": "node-tuning-operator",
                "namespace": "openshift-cluster-node-tuning-operator",
                "ownerReferences": [
                    {
                        "apiVersion": "config.openshift.io/v1",
                        "controller": true,
                        "kind": "ClusterVersion",
                        "name": "version",
                        "uid": "46b5b63c-dc72-48b6-9f35-f2dea4314c26"
                    }
                ],
                "resourceVersion": "1248",
                "uid": "bde0d68f-f533-4339-b1a9-a26d6730c45d"
            },
            "spec": {
                "groups": [
                    {
                        "name": "node-tuning-operator.rules",
                        "rules": [
                            {
                                "alert": "NTOPodsNotReady",
                                "annotations": {
                                    "description": "Pod {{ $labels.pod }} is not ready.\nReview the \"Event\" objects in \"openshift-cluster-node-tuning-operator\" namespace for further details.\n",
                                    "summary": "Pod {{ $labels.pod }} is not ready."
                                },
                                "expr": "kube_pod_status_ready{namespace='openshift-cluster-node-tuning-operator', condition='true'} == 0\n",
                                "for": "30m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "NTODegraded",
                                "annotations": {
                                    "description": "The Node Tuning Operator is degraded. Review the \"node-tuning\" ClusterOperator object for further details.",
                                    "summary": "The Node Tuning Operator is degraded."
                                },
                                "expr": "nto_degraded_info == 1",
                                "for": "2h",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "expr": "count by (_id) (nto_profile_calculated_total{profile!~\"openshift-node\",profile!~\"openshift-control-plane\",profile!~\"openshift\"})",
                                "record": "nto_custom_profiles:count"
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "annotations": {
                    "capability.openshift.io/name": "openshift-samples",
                    "include.release.openshift.io/ibm-cloud-managed": "true",
                    "include.release.openshift.io/self-managed-high-availability": "true"
                },
                "creationTimestamp": "2025-12-02T13:23:16Z",
                "generation": 1,
                "labels": {
                    "name": "samples-operator-alerts"
                },
                "name": "samples-operator-alerts",
                "namespace": "openshift-cluster-samples-operator",
                "ownerReferences": [
                    {
                        "apiVersion": "config.openshift.io/v1",
                        "controller": true,
                        "kind": "ClusterVersion",
                        "name": "version",
                        "uid": "46b5b63c-dc72-48b6-9f35-f2dea4314c26"
                    }
                ],
                "resourceVersion": "1192",
                "uid": "edaf9b7a-3f03-4132-8687-cd8e291f5abe"
            },
            "spec": {
                "groups": [
                    {
                        "name": "SamplesOperator",
                        "rules": [
                            {
                                "alert": "SamplesRetriesMissingOnImagestreamImportFailing",
                                "annotations": {
                                    "description": "Samples operator is detecting problems with imagestream image imports, and the periodic retries of those\nimports are not occurring.  Contact support.  You can look at the \"openshift-samples\" ClusterOperator object\nfor details. Most likely there are issues with the external image registry hosting the images that need to\nbe investigated.  The list of ImageStreams that have failing imports are:\n{{ range query \"openshift_samples_failed_imagestream_import_info \u003e 0\" }}\n  {{ .Labels.name }}\n{{ end }}\nHowever, the list of ImageStreams for which samples operator is retrying imports is:\nretrying imports:\n{{ range query \"openshift_samples_retry_imagestream_import_total \u003e 0\" }}\n   {{ .Labels.imagestreamname }}\n{{ end }}\n",
                                    "summary": "Samples operator is having problems with imagestream imports and its retries."
                                },
                                "expr": "sum(openshift_samples_failed_imagestream_import_info) \u003e sum(openshift_samples_retry_imagestream_import_total) - sum(openshift_samples_retry_imagestream_import_total offset 30m)",
                                "for": "2h",
                                "labels": {
                                    "namespace": "openshift-cluster-samples-operator",
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "SamplesImagestreamImportFailing",
                                "annotations": {
                                    "description": "Samples operator is detecting problems with imagestream image imports.  You can look at the \"openshift-samples\"\nClusterOperator object for details. Most likely there are issues with the external image registry hosting\nthe images that needs to be investigated.  Or you can consider marking samples operator Removed if you do not\ncare about having sample imagestreams available.  The list of ImageStreams for which samples operator is\nretrying imports:\n{{ range query \"openshift_samples_retry_imagestream_import_total \u003e 0\" }}\n   {{ .Labels.imagestreamname }}\n{{ end }}\n",
                                    "summary": "Samples operator is detecting problems with imagestream image imports"
                                },
                                "expr": "sum(openshift_samples_retry_imagestream_import_total) - sum(openshift_samples_retry_imagestream_import_total offset 30m) \u003e sum(openshift_samples_failed_imagestream_import_info)",
                                "for": "2h",
                                "labels": {
                                    "namespace": "openshift-cluster-samples-operator",
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "SamplesDegraded",
                                "annotations": {
                                    "description": "Samples could not be deployed and the operator is degraded. Review the \"openshift-samples\" ClusterOperator object for further details.\n",
                                    "summary": "Samples operator is degraded."
                                },
                                "expr": "openshift_samples_degraded_info == 1",
                                "for": "2h",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "SamplesInvalidConfig",
                                "annotations": {
                                    "description": "Samples operator has been given an invalid configuration.\n",
                                    "summary": "Samples operator Invalid configuration"
                                },
                                "expr": "openshift_samples_invalidconfig_info == 1",
                                "for": "2h",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "SamplesMissingSecret",
                                "annotations": {
                                    "description": "Samples operator cannot find the samples pull secret in the openshift namespace.\n",
                                    "summary": "Samples operator is not able to find secret"
                                },
                                "expr": "openshift_samples_invalidsecret_info{reason=\"missing_secret\"} == 1",
                                "for": "2h",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "SamplesMissingTBRCredential",
                                "annotations": {
                                    "description": "The samples operator cannot find credentials for 'registry.redhat.io'. Many of the sample ImageStreams will fail to import unless the 'samplesRegistry' in the operator configuration is changed.\n",
                                    "summary": "Samples operator is not able to find the credentials for registry"
                                },
                                "expr": "openshift_samples_invalidsecret_info{reason=\"missing_tbr_credential\"} == 1",
                                "for": "2h",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "SamplesTBRInaccessibleOnBoot",
                                "annotations": {
                                    "description": "One of two situations has occurred.  Either\nsamples operator could not access 'registry.redhat.io' during its initial installation and it bootstrapped as removed.\nIf this is expected, and stems from installing in a restricted network environment, please note that if you\nplan on mirroring images associated with sample imagestreams into a registry available in your restricted\nnetwork environment, and subsequently moving samples operator back to 'Managed' state, a list of the images\nassociated with each image stream tag from the samples catalog is\nprovided in the 'imagestreamtag-to-image' config map in the 'openshift-cluster-samples-operator' namespace to\nassist the mirroring process.\nOr, the use of allowed registries or blocked registries with global imagestream configuration will not allow\nsamples operator to create imagestreams using the default image registry 'registry.redhat.io'.\n",
                                    "summary": "Samples operator is not able to access the registry on boot"
                                },
                                "expr": "openshift_samples_tbr_inaccessible_info == 1",
                                "for": "2d",
                                "labels": {
                                    "severity": "info"
                                }
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "annotations": {
                    "capability.openshift.io/name": "Storage",
                    "include.release.openshift.io/hypershift": "true",
                    "include.release.openshift.io/ibm-cloud-managed": "true",
                    "include.release.openshift.io/self-managed-high-availability": "true",
                    "include.release.openshift.io/single-node-developer": "true"
                },
                "creationTimestamp": "2025-12-02T13:23:22Z",
                "generation": 1,
                "labels": {
                    "role": "alert-rules"
                },
                "name": "prometheus",
                "namespace": "openshift-cluster-storage-operator",
                "ownerReferences": [
                    {
                        "apiVersion": "config.openshift.io/v1",
                        "controller": true,
                        "kind": "ClusterVersion",
                        "name": "version",
                        "uid": "46b5b63c-dc72-48b6-9f35-f2dea4314c26"
                    }
                ],
                "resourceVersion": "1373",
                "uid": "5830e2dc-550d-4e8d-894b-487a11008046"
            },
            "spec": {
                "groups": [
                    {
                        "name": "default-storage-classes.rules",
                        "rules": [
                            {
                                "alert": "MultipleDefaultStorageClasses",
                                "annotations": {
                                    "description": "Cluster storage operator monitors all storage classes configured in the cluster\nand checks there is not more than one default StorageClass configured.\n",
                                    "message": "StorageClass count check is failing (there should not be more than one default StorageClass)",
                                    "summary": "More than one default StorageClass detected."
                                },
                                "expr": "min_over_time(default_storage_class_count[5m]) \u003e 1",
                                "for": "10m",
                                "labels": {
                                    "severity": "warning"
                                }
                            }
                        ]
                    },
                    {
                        "name": "storage-operations.rules",
                        "rules": [
                            {
                                "alert": "PodStartupStorageOperationsFailing",
                                "annotations": {
                                    "description": "Failing storage operation \"{{ $labels.operation_name }}\" of volume plugin {{ $labels.volume_plugin }} was preventing Pods on node {{ $labels.node }}\nfrom starting for past 5 minutes.\nPlease investigate Pods that are \"ContainerCreating\" on the node: \"oc get pod --field-selector=spec.nodeName={{ $labels.node }} --all-namespaces | grep ContainerCreating\".\nEvents of the Pods should contain exact error message: \"oc describe pod -n \u003cpod namespace\u003e \u003cpod name\u003e\".\n",
                                    "summary": "Pods can't start because {{ $labels.operation_name }} of volume plugin {{ $labels.volume_plugin }} is permanently failing on node {{ $labels.node }}."
                                },
                                "expr": "increase(storage_operation_duration_seconds_count{status != \"success\", operation_name =~\"volume_attach|volume_mount\"}[5m]) \u003e 0\n  and ignoring(status) (sum without(status)\n    (increase(storage_operation_duration_seconds_count{status = \"success\", operation_name =~\"volume_attach|volume_mount\"}[5m])\n      or increase(storage_operation_duration_seconds_count{status != \"success\", operation_name =~\"volume_attach|volume_mount\"}[5m]) * 0)\n    ) == 0\n",
                                "for": "5m",
                                "labels": {
                                    "severity": "info"
                                }
                            }
                        ]
                    },
                    {
                        "name": "storage-selinux.rules",
                        "rules": [
                            {
                                "expr": "sum(volume_manager_selinux_pod_context_mismatch_warnings_total) + sum(volume_manager_selinux_pod_context_mismatch_errors_total)",
                                "record": "cluster:volume_manager_selinux_pod_context_mismatch_total"
                            },
                            {
                                "expr": "sum by(volume_plugin) (volume_manager_selinux_volume_context_mismatch_warnings_total{volume_plugin !~\".*-e2e-.*\"})",
                                "record": "cluster:volume_manager_selinux_volume_context_mismatch_warnings_total"
                            },
                            {
                                "expr": "sum by(volume_plugin) (volume_manager_selinux_volume_context_mismatch_errors_total{volume_plugin !~\".*-e2e-.*\"})",
                                "record": "cluster:volume_manager_selinux_volume_context_mismatch_errors_total"
                            },
                            {
                                "expr": "sum by(volume_plugin) (volume_manager_selinux_volumes_admitted_total{volume_plugin !~\".*-e2e-.*\"})",
                                "record": "cluster:volume_manager_selinux_volumes_admitted_total"
                            }
                        ]
                    },
                    {
                        "name": "kubernetes-storage",
                        "rules": [
                            {
                                "alert": "KubePersistentVolumeFillingUp",
                                "annotations": {
                                    "description": "The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} is only {{ $value | humanizePercentage }} free.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeFillingUp.md",
                                    "summary": "PersistentVolume is filling up."
                                },
                                "expr": "(\n  kubelet_volume_stats_available_bytes{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kubelet\", metrics_path=\"/metrics\"}\n    /\n  kubelet_volume_stats_capacity_bytes{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kubelet\", metrics_path=\"/metrics\"}\n) \u003c 0.03\nand\nkubelet_volume_stats_used_bytes{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kubelet\", metrics_path=\"/metrics\"} \u003e 0\nunless on(cluster, namespace, persistentvolumeclaim)\nkube_persistentvolumeclaim_access_mode{namespace=~\"(openshift-.*|kube-.*|default)\", access_mode=\"ReadOnlyMany\"} == 1\nunless on(cluster, namespace, persistentvolumeclaim)\nkube_persistentvolumeclaim_labels{namespace=~\"(openshift-.*|kube-.*|default)\",label_alerts_k8s_io_kube_persistent_volume_filling_up=\"disabled\"} == 1\n",
                                "for": "1m",
                                "labels": {
                                    "severity": "critical"
                                }
                            },
                            {
                                "alert": "KubePersistentVolumeFillingUp",
                                "annotations": {
                                    "description": "Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} is expected to fill up within four days. Currently {{ $value | humanizePercentage }} is available.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeFillingUp.md",
                                    "summary": "PersistentVolume is filling up."
                                },
                                "expr": "(\n  kubelet_volume_stats_available_bytes{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kubelet\", metrics_path=\"/metrics\"}\n    /\n  kubelet_volume_stats_capacity_bytes{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kubelet\", metrics_path=\"/metrics\"}\n) \u003c 0.15\nand\nkubelet_volume_stats_used_bytes{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kubelet\", metrics_path=\"/metrics\"} \u003e 0\nand\npredict_linear(kubelet_volume_stats_available_bytes{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kubelet\", metrics_path=\"/metrics\"}[6h], 4 * 24 * 3600) \u003c 0\nunless on(cluster, namespace, persistentvolumeclaim)\nkube_persistentvolumeclaim_access_mode{namespace=~\"(openshift-.*|kube-.*|default)\", access_mode=\"ReadOnlyMany\"} == 1\nunless on(cluster, namespace, persistentvolumeclaim)\nkube_persistentvolumeclaim_labels{namespace=~\"(openshift-.*|kube-.*|default)\",label_alerts_k8s_io_kube_persistent_volume_filling_up=\"disabled\"} == 1\n",
                                "for": "1h",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubePersistentVolumeInodesFillingUp",
                                "annotations": {
                                    "description": "The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} only has {{ $value | humanizePercentage }} free inodes.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeInodesFillingUp.md",
                                    "summary": "PersistentVolumeInodes are filling up."
                                },
                                "expr": "(\n  kubelet_volume_stats_inodes_free{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kubelet\", metrics_path=\"/metrics\"}\n    /\n  kubelet_volume_stats_inodes{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kubelet\", metrics_path=\"/metrics\"}\n) \u003c 0.03\nand\nkubelet_volume_stats_inodes_used{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kubelet\", metrics_path=\"/metrics\"} \u003e 0\nunless on(cluster, namespace, persistentvolumeclaim)\nkube_persistentvolumeclaim_access_mode{namespace=~\"(openshift-.*|kube-.*|default)\", access_mode=\"ReadOnlyMany\"} == 1\nunless on(cluster, namespace, persistentvolumeclaim)\nkube_persistentvolumeclaim_labels{namespace=~\"(openshift-.*|kube-.*|default)\",label_alerts_k8s_io_kube_persistent_volume_filling_up=\"disabled\"} == 1\n",
                                "for": "1m",
                                "labels": {
                                    "severity": "critical"
                                }
                            },
                            {
                                "alert": "KubePersistentVolumeInodesFillingUp",
                                "annotations": {
                                    "description": "Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} is expected to run out of inodes within four days. Currently {{ $value | humanizePercentage }} of its inodes are free.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePersistentVolumeInodesFillingUp.md",
                                    "summary": "PersistentVolumeInodes are filling up."
                                },
                                "expr": "(\n  kubelet_volume_stats_inodes_free{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kubelet\", metrics_path=\"/metrics\"}\n    /\n  kubelet_volume_stats_inodes{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kubelet\", metrics_path=\"/metrics\"}\n) \u003c 0.15\nand\nkubelet_volume_stats_inodes_used{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kubelet\", metrics_path=\"/metrics\"} \u003e 0\nand\npredict_linear(kubelet_volume_stats_inodes_free{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kubelet\", metrics_path=\"/metrics\"}[6h], 4 * 24 * 3600) \u003c 0\nunless on(cluster, namespace, persistentvolumeclaim)\nkube_persistentvolumeclaim_access_mode{namespace=~\"(openshift-.*|kube-.*|default)\", access_mode=\"ReadOnlyMany\"} == 1\nunless on(cluster, namespace, persistentvolumeclaim)\nkube_persistentvolumeclaim_labels{namespace=~\"(openshift-.*|kube-.*|default)\",label_alerts_k8s_io_kube_persistent_volume_filling_up=\"disabled\"} == 1\n",
                                "for": "1h",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubePersistentVolumeErrors",
                                "annotations": {
                                    "description": "The persistent volume {{ $labels.persistentvolume }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} has status {{ $labels.phase }}.",
                                    "summary": "PersistentVolume is having issues with provisioning."
                                },
                                "expr": "kube_persistentvolume_status_phase{phase=~\"Failed|Pending\",namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"} \u003e 0\n",
                                "for": "5m",
                                "labels": {
                                    "severity": "warning"
                                }
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "annotations": {
                    "capability.openshift.io/name": "Console",
                    "include.release.openshift.io/hypershift": "true",
                    "include.release.openshift.io/ibm-cloud-managed": "true",
                    "include.release.openshift.io/self-managed-high-availability": "true",
                    "include.release.openshift.io/single-node-developer": "true"
                },
                "creationTimestamp": "2025-12-02T13:30:09Z",
                "generation": 1,
                "labels": {
                    "prometheus": "k8s"
                },
                "name": "cluster-monitoring-prometheus-rules",
                "namespace": "openshift-console-operator",
                "ownerReferences": [
                    {
                        "apiVersion": "config.openshift.io/v1",
                        "controller": true,
                        "kind": "ClusterVersion",
                        "name": "version",
                        "uid": "46b5b63c-dc72-48b6-9f35-f2dea4314c26"
                    }
                ],
                "resourceVersion": "9638",
                "uid": "d5d6507b-5009-49b6-b241-04244d203b0b"
            },
            "spec": {
                "groups": [
                    {
                        "name": "openshift/console-operator",
                        "rules": [
                            {
                                "expr": "sum(console_auth_login_requests_total)",
                                "record": "cluster:console_auth_login_requests_total:sum"
                            },
                            {
                                "expr": "sum(console_auth_login_successes_total) by (role)",
                                "record": "cluster:console_auth_login_successes_total:sum"
                            },
                            {
                                "expr": "sum(console_auth_login_failures_total) by (reason)",
                                "record": "cluster:console_auth_login_failures_total:sum"
                            },
                            {
                                "expr": "sum(console_auth_logout_requests_total) by (reason)",
                                "record": "cluster:console_auth_logout_requests_total:sum"
                            },
                            {
                                "expr": "max(console_usage_users) by (role)",
                                "record": "cluster:console_usage_users:max"
                            },
                            {
                                "expr": "max(console_plugins_info) by (name, state)",
                                "record": "cluster:console_plugins_info:max"
                            },
                            {
                                "expr": "max(console_customization_perspectives_info) by (name, state)",
                                "record": "cluster:console_customization_perspectives_info:max"
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "annotations": {
                    "include.release.openshift.io/ibm-cloud-managed": "true",
                    "include.release.openshift.io/self-managed-high-availability": "true",
                    "include.release.openshift.io/single-node-developer": "true"
                },
                "creationTimestamp": "2025-12-02T13:23:18Z",
                "generation": 1,
                "labels": {
                    "role": "alert-rules"
                },
                "name": "dns",
                "namespace": "openshift-dns-operator",
                "ownerReferences": [
                    {
                        "apiVersion": "config.openshift.io/v1",
                        "controller": true,
                        "kind": "ClusterVersion",
                        "name": "version",
                        "uid": "46b5b63c-dc72-48b6-9f35-f2dea4314c26"
                    }
                ],
                "resourceVersion": "1246",
                "uid": "d761c1e4-1c73-422c-b3e8-274cb0bb6de6"
            },
            "spec": {
                "groups": [
                    {
                        "name": "openshift-dns.rules",
                        "rules": [
                            {
                                "alert": "CoreDNSPanicking",
                                "annotations": {
                                    "description": "{{ $value }} CoreDNS panics observed on {{ $labels.instance }}",
                                    "summary": "CoreDNS panic"
                                },
                                "expr": "increase(coredns_panics_total[10m]) \u003e 0",
                                "for": "5m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "CoreDNSHealthCheckSlow",
                                "annotations": {
                                    "description": "CoreDNS Health Checks are slowing down (instance {{ $labels.instance }})",
                                    "summary": "CoreDNS health checks"
                                },
                                "expr": "histogram_quantile(.95, sum(rate(coredns_health_request_duration_seconds_bucket[5m])) by (instance, le)) \u003e 10",
                                "for": "5m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "CoreDNSErrorsHigh",
                                "annotations": {
                                    "description": "CoreDNS is returning SERVFAIL for {{ $value | humanizePercentage }} of requests.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-dns-operator/CoreDNSErrorsHigh.md",
                                    "summary": "CoreDNS serverfail"
                                },
                                "expr": "(sum by(namespace) (rate(coredns_dns_responses_total{rcode=\"SERVFAIL\"}[5m]))\n  /\nsum by(namespace) (rate(coredns_dns_responses_total[5m])))\n\u003e 0.01\n",
                                "for": "5m",
                                "labels": {
                                    "severity": "warning"
                                }
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "creationTimestamp": "2025-12-02T13:48:29Z",
                "generation": 1,
                "name": "gitops-operator-argocd-alerts",
                "namespace": "openshift-gitops",
                "ownerReferences": [
                    {
                        "apiVersion": "argoproj.io/v1beta1",
                        "blockOwnerDeletion": true,
                        "controller": true,
                        "kind": "ArgoCD",
                        "name": "openshift-gitops",
                        "uid": "b7bc9531-0a67-4f55-9278-b631414773ea"
                    }
                ],
                "resourceVersion": "20291",
                "uid": "d13b39ac-c00d-4834-aca8-258945c9fe9e"
            },
            "spec": {
                "groups": [
                    {
                        "name": "GitOpsOperatorArgoCD",
                        "rules": [
                            {
                                "alert": "ArgoCDSyncAlert",
                                "annotations": {
                                    "description": "Argo CD application {{ $labels.name }} is out of sync. Check ArgoCDSyncAlert status, this alert is designed to notify that an application managed by Argo CD is out of sync.",
                                    "summary": "Argo CD application is out of sync"
                                },
                                "expr": "argocd_app_info{namespace=\"openshift-gitops\",sync_status=\"OutOfSync\"} \u003e 0",
                                "for": "5m",
                                "labels": {
                                    "severity": "warning"
                                }
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "annotations": {
                    "capability.openshift.io/name": "ImageRegistry",
                    "include.release.openshift.io/hypershift": "true",
                    "include.release.openshift.io/ibm-cloud-managed": "true",
                    "include.release.openshift.io/self-managed-high-availability": "true",
                    "include.release.openshift.io/single-node-developer": "true"
                },
                "creationTimestamp": "2025-12-02T13:23:25Z",
                "generation": 1,
                "name": "image-registry-operator-alerts",
                "namespace": "openshift-image-registry",
                "ownerReferences": [
                    {
                        "apiVersion": "config.openshift.io/v1",
                        "controller": true,
                        "kind": "ClusterVersion",
                        "name": "version",
                        "uid": "46b5b63c-dc72-48b6-9f35-f2dea4314c26"
                    }
                ],
                "resourceVersion": "1791",
                "uid": "3848c96a-ca93-42d2-94b2-08734c527ace"
            },
            "spec": {
                "groups": [
                    {
                        "name": "pvc-problem-detector.rules",
                        "rules": [
                            {
                                "alert": "ImageRegistryStorageReadOnly",
                                "annotations": {
                                    "description": "The image registry storage is read-only. Read-only storage affects direct pushes to the image registry, and pull-through proxy caching. In the case of pull-through proxy caching, read-only storage is particularly important because without it the image registry won't be actually caching anything. Please verify your backing storage solution and make sure the volume mounted on the image-registry pods is writable to avoid potential outages.",
                                    "message": "The image registry storage is read-only and no images will be committed to storage.",
                                    "summary": "The image registry storage is read-only and no images will be committed to storage."
                                },
                                "expr": "sum without(instance, pod, operation) (rate(imageregistry_storage_errors_total{code=\"READ_ONLY_FILESYSTEM\"}[5m])) \u003e 0",
                                "for": "10m",
                                "labels": {
                                    "kubernetes_operator_part_of": "image-registry",
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "ImageRegistryStorageFull",
                                "annotations": {
                                    "description": "The image registry storage disk is full. A full disk affects direct pushes to the image registry, and pull-through proxy caching. In the case of pull-through proxy caching, disk space is particularly important because without it the image registry won't be actually caching anything. Please verify your backing storage solution and make sure the volume mounted on the image-registry pods have enough free disk space to avoid potential outages.",
                                    "message": "The image registry storage disk is full and no images will be committed to storage.",
                                    "summary": "The image registry storage disk is full and no images will be committed to storage."
                                },
                                "expr": "sum without(instance, pod, operation) (rate(imageregistry_storage_errors_total{code=\"DEVICE_OUT_OF_SPACE\"}[5m])) \u003e 0",
                                "for": "10m",
                                "labels": {
                                    "kubernetes_operator_part_of": "image-registry",
                                    "severity": "warning"
                                }
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "annotations": {
                    "capability.openshift.io/name": "ImageRegistry",
                    "include.release.openshift.io/hypershift": "true",
                    "include.release.openshift.io/ibm-cloud-managed": "true",
                    "include.release.openshift.io/self-managed-high-availability": "true",
                    "include.release.openshift.io/single-node-developer": "true"
                },
                "creationTimestamp": "2025-12-02T13:23:25Z",
                "generation": 1,
                "name": "image-registry-rules",
                "namespace": "openshift-image-registry",
                "ownerReferences": [
                    {
                        "apiVersion": "config.openshift.io/v1",
                        "controller": true,
                        "kind": "ClusterVersion",
                        "name": "version",
                        "uid": "46b5b63c-dc72-48b6-9f35-f2dea4314c26"
                    }
                ],
                "resourceVersion": "1779",
                "uid": "8e1261c0-39e9-4c96-ad29-8a6bedb476c4"
            },
            "spec": {
                "groups": [
                    {
                        "name": "imageregistry.operations.rules",
                        "rules": [
                            {
                                "expr": "label_replace(\n  label_replace(\n    sum by (operation) (imageregistry_request_duration_seconds_count{operation=\"BlobStore.ServeBlob\"}), \"operation\", \"get\", \"operation\", \"(.+)\"\n  ), \"resource_type\", \"blob\", \"resource_type\", \"\"\n)\n",
                                "record": "imageregistry:operations_count:sum"
                            },
                            {
                                "expr": "label_replace(\n  label_replace(\n    sum by (operation) (imageregistry_request_duration_seconds_count{operation=\"BlobStore.Create\"}), \"operation\", \"create\", \"operation\", \"(.+)\"\n  ), \"resource_type\", \"blob\", \"resource_type\", \"\"\n)\n",
                                "record": "imageregistry:operations_count:sum"
                            },
                            {
                                "expr": "label_replace(\n  label_replace(\n    sum by (operation) (imageregistry_request_duration_seconds_count{operation=\"ManifestService.Get\"}), \"operation\", \"get\", \"operation\", \"(.+)\"\n  ), \"resource_type\", \"manifest\", \"resource_type\", \"\"\n)\n",
                                "record": "imageregistry:operations_count:sum"
                            },
                            {
                                "expr": "label_replace(\n  label_replace(\n    sum by (operation) (imageregistry_request_duration_seconds_count{operation=\"ManifestService.Put\"}), \"operation\", \"create\", \"operation\", \"(.+)\"\n  ), \"resource_type\", \"manifest\", \"resource_type\", \"\"\n)\n",
                                "record": "imageregistry:operations_count:sum"
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "annotations": {
                    "capability.openshift.io/name": "ImageRegistry",
                    "include.release.openshift.io/hypershift": "true",
                    "include.release.openshift.io/ibm-cloud-managed": "true",
                    "include.release.openshift.io/self-managed-high-availability": "true",
                    "include.release.openshift.io/single-node-developer": "true"
                },
                "creationTimestamp": "2025-12-02T13:23:25Z",
                "generation": 1,
                "name": "imagestreams-rules",
                "namespace": "openshift-image-registry",
                "ownerReferences": [
                    {
                        "apiVersion": "config.openshift.io/v1",
                        "controller": true,
                        "kind": "ClusterVersion",
                        "name": "version",
                        "uid": "46b5b63c-dc72-48b6-9f35-f2dea4314c26"
                    }
                ],
                "resourceVersion": "1765",
                "uid": "24f4c0ec-06a6-451d-8abc-e9801f7cf18e"
            },
            "spec": {
                "groups": [
                    {
                        "name": "imagestreams.rules",
                        "rules": [
                            {
                                "expr": "sum by (location, source) (image_registry_image_stream_tags_total)",
                                "record": "imageregistry:imagestreamtags_count:sum"
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "annotations": {
                    "capability.openshift.io/name": "Ingress",
                    "include.release.openshift.io/ibm-cloud-managed": "true",
                    "include.release.openshift.io/self-managed-high-availability": "true",
                    "include.release.openshift.io/single-node-developer": "true"
                },
                "creationTimestamp": "2025-12-02T13:23:45Z",
                "generation": 1,
                "labels": {
                    "role": "alert-rules"
                },
                "name": "ingress-operator",
                "namespace": "openshift-ingress-operator",
                "ownerReferences": [
                    {
                        "apiVersion": "config.openshift.io/v1",
                        "controller": true,
                        "kind": "ClusterVersion",
                        "name": "version",
                        "uid": "46b5b63c-dc72-48b6-9f35-f2dea4314c26"
                    }
                ],
                "resourceVersion": "2871",
                "uid": "70973762-642e-4157-8a30-17daa245e85c"
            },
            "spec": {
                "groups": [
                    {
                        "name": "openshift-ingress.rules",
                        "rules": [
                            {
                                "alert": "HAProxyReloadFail",
                                "annotations": {
                                    "description": "This alert fires when HAProxy fails to reload its configuration, which will result in the router not picking up recently created or modified routes.",
                                    "message": "HAProxy reloads are failing on {{ $labels.pod }}. Router is not respecting recently created or modified routes",
                                    "summary": "HAProxy reload failure"
                                },
                                "expr": "template_router_reload_failure == 1",
                                "for": "5m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "HAProxyDown",
                                "annotations": {
                                    "description": "This alert fires when metrics report that HAProxy is down.",
                                    "message": "HAProxy metrics are reporting that HAProxy is down on pod {{ $labels.namespace }} / {{ $labels.pod }}",
                                    "summary": "HAProxy is down"
                                },
                                "expr": "haproxy_up == 0",
                                "for": "5m",
                                "labels": {
                                    "severity": "critical"
                                }
                            },
                            {
                                "alert": "IngressControllerDegraded",
                                "annotations": {
                                    "description": "This alert fires when the IngressController status is degraded.",
                                    "message": "The {{ $labels.namespace }}/{{ $labels.name }} ingresscontroller is\ndegraded: {{ $labels.reason }}.\n",
                                    "summary": "IngressController is degraded"
                                },
                                "expr": "ingress_controller_conditions{condition=\"Degraded\"} == 1",
                                "for": "5m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "IngressControllerUnavailable",
                                "annotations": {
                                    "description": "This alert fires when the IngressController is not available.",
                                    "message": "The {{ $labels.namespace }}/{{ $labels.name }} ingresscontroller is\nunavailable: {{ $labels.reason }}.\n",
                                    "summary": "IngressController is unavailable"
                                },
                                "expr": "ingress_controller_conditions{condition=\"Available\"} == 0",
                                "for": "5m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "expr": "min(route_metrics_controller_routes_per_shard)",
                                "record": "cluster:route_metrics_controller_routes_per_shard:min"
                            },
                            {
                                "expr": "max(route_metrics_controller_routes_per_shard)",
                                "record": "cluster:route_metrics_controller_routes_per_shard:max"
                            },
                            {
                                "expr": "avg(route_metrics_controller_routes_per_shard)",
                                "record": "cluster:route_metrics_controller_routes_per_shard:avg"
                            },
                            {
                                "expr": "quantile(0.5, route_metrics_controller_routes_per_shard)",
                                "record": "cluster:route_metrics_controller_routes_per_shard:median"
                            },
                            {
                                "expr": "sum (openshift_route_info) by (tls_termination)",
                                "record": "cluster:openshift_route_info:tls_termination:sum"
                            }
                        ]
                    },
                    {
                        "name": "openshift-ingress-to-route-controller.rules",
                        "rules": [
                            {
                                "alert": "IngressWithoutClassName",
                                "annotations": {
                                    "description": "This alert fires when there is an Ingress with an unset IngressClassName for longer than one day.",
                                    "message": "Ingress {{ $labels.namespace }}/{{ $labels.name }} is missing the IngressClassName for 1 day.",
                                    "summary": "Ingress without IngressClassName for 1 day"
                                },
                                "expr": "openshift_ingress_to_route_controller_ingress_without_class_name == 1",
                                "for": "1d",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "UnmanagedRoutes",
                                "annotations": {
                                    "description": "This alert fires when there is a Route owned by an unmanaged Ingress.",
                                    "message": "Route {{ $labels.namespace }}/{{ $labels.name }} is owned by an unmanaged Ingress.",
                                    "summary": "Route owned by an Ingress no longer managed"
                                },
                                "expr": "openshift_ingress_to_route_controller_route_with_unmanaged_owner == 1",
                                "for": "1h",
                                "labels": {
                                    "severity": "warning"
                                }
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "creationTimestamp": "2025-12-02T13:29:28Z",
                "generation": 1,
                "name": "insights-prometheus-rules",
                "namespace": "openshift-insights",
                "resourceVersion": "8499",
                "uid": "d52b5c9a-9466-4fe9-b7e6-98ffd7194398"
            },
            "spec": {
                "groups": [
                    {
                        "name": "insights",
                        "rules": [
                            {
                                "alert": "InsightsDisabled",
                                "annotations": {
                                    "description": "Insights operator is disabled. In order to enable Insights and benefit from recommendations specific to your cluster, please follow steps listed in the documentation: https://docs.openshift.com/container-platform/latest/support/remote_health_monitoring/enabling-remote-health-reporting.html",
                                    "summary": "Insights operator is disabled."
                                },
                                "expr": "max without (job, pod, service, instance) (cluster_operator_conditions{name=\"insights\", condition=\"Disabled\"} == 1)",
                                "for": "5m",
                                "labels": {
                                    "namespace": "openshift-insights",
                                    "severity": "info"
                                }
                            },
                            {
                                "alert": "SimpleContentAccessNotAvailable",
                                "annotations": {
                                    "description": "Simple content access (SCA) is not enabled. Once enabled, Insights Operator can automatically import the SCA certificates from Red Hat OpenShift Cluster Manager making it easier to use the content provided by your Red Hat subscriptions when creating container images. See https://docs.openshift.com/container-platform/latest/cicd/builds/running-entitled-builds.html for more information.",
                                    "summary": "Simple content access certificates are not available."
                                },
                                "expr": " max without (job, pod, service, instance) (max_over_time(cluster_operator_conditions{name=\"insights\", condition=\"SCAAvailable\", reason=\"NotFound\"}[5m]) == 0)",
                                "for": "5m",
                                "labels": {
                                    "namespace": "openshift-insights",
                                    "severity": "info"
                                }
                            },
                            {
                                "alert": "InsightsRecommendationActive",
                                "annotations": {
                                    "description": "Insights recommendation \"{{ $labels.description }}\" with total risk \"{{ $labels.total_risk }}\" was detected on the cluster. More information is available at {{ $labels.info_link }}.",
                                    "summary": "An Insights recommendation is active for this cluster."
                                },
                                "expr": "insights_recommendation_active == 1",
                                "for": "5m",
                                "labels": {
                                    "severity": "info"
                                }
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "creationTimestamp": "2025-12-02T13:23:28Z",
                "generation": 1,
                "labels": {
                    "hypershift.openshift.io/managed": "true"
                },
                "name": "api-usage",
                "namespace": "openshift-kube-apiserver",
                "resourceVersion": "1977",
                "uid": "e648a7f9-a05a-4ff4-9bbf-1c1372e67cea"
            },
            "spec": {
                "groups": [
                    {
                        "name": "pre-release-lifecycle",
                        "rules": [
                            {
                                "alert": "APIRemovedInNextReleaseInUse",
                                "annotations": {
                                    "description": "Deprecated API that will be removed in the next version is being used. Removing the workload that is using the {{ $labels.group }}.{{ $labels.version }}/{{ $labels.resource }} API might be necessary for a successful upgrade to the next cluster version. Refer to `oc get apirequestcounts {{ $labels.resource }}.{{ $labels.version }}.{{ $labels.group }} -o yaml` to identify the workload.",
                                    "summary": "Deprecated API that will be removed in the next version is being used."
                                },
                                "expr": "group(apiserver_requested_deprecated_apis{removed_release=\"1.24\"}) by (group,version,resource) and (sum by(group,version,resource) (rate(apiserver_request_total{system_client!=\"kube-controller-manager\",system_client!=\"cluster-policy-controller\"}[4h]))) \u003e 0\n",
                                "for": "1h",
                                "labels": {
                                    "namespace": "openshift-kube-apiserver",
                                    "severity": "info"
                                }
                            },
                            {
                                "alert": "APIRemovedInNextEUSReleaseInUse",
                                "annotations": {
                                    "description": "Deprecated API that will be removed in the next EUS version is being used. Removing the workload that is using the {{ $labels.group }}.{{ $labels.version }}/{{ $labels.resource }} API might be necessary for a successful upgrade to the next EUS cluster version. Refer to `oc get apirequestcounts {{ $labels.resource }}.{{ $labels.version }}.{{ $labels.group }} -o yaml` to identify the workload.",
                                    "summary": "Deprecated API that will be removed in the next EUS version is being used."
                                },
                                "expr": "group(apiserver_requested_deprecated_apis{removed_release=~\"1\\\\.2[45]\"}) by (group,version,resource) and (sum by(group,version,resource) (rate(apiserver_request_total{system_client!=\"kube-controller-manager\",system_client!=\"cluster-policy-controller\"}[4h]))) \u003e 0\n",
                                "for": "1h",
                                "labels": {
                                    "namespace": "openshift-kube-apiserver",
                                    "severity": "info"
                                }
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "creationTimestamp": "2025-12-02T13:23:28Z",
                "generation": 1,
                "labels": {
                    "hypershift.openshift.io/managed": "true"
                },
                "name": "podsecurity",
                "namespace": "openshift-kube-apiserver",
                "resourceVersion": "1978",
                "uid": "90992bed-297a-400d-905c-54b7805abcd2"
            },
            "spec": {
                "groups": [
                    {
                        "name": "pod-security-violation",
                        "rules": [
                            {
                                "alert": "PodSecurityViolation",
                                "annotations": {
                                    "description": "A workload (pod, deployment, daemonset, ...) was created somewhere in the cluster but it did not match the PodSecurity \"{{ $labels.policy_level }}\" profile defined by its namespace either via the cluster-wide configuration (which triggers on a \"restricted\" profile violations) or by the namespace local Pod Security labels. Refer to Kubernetes documentation on Pod Security Admission to learn more about these violations.",
                                    "summary": "One or more workloads users created in the cluster don't match their Pod Security profile"
                                },
                                "expr": "sum(increase(pod_security_evaluations_total{decision=\"deny\",mode=\"audit\",resource=\"pod\",ocp_namespace=\"\"}[1d])) by (policy_level, ocp_namespace) \u003e 0\n",
                                "labels": {
                                    "namespace": "openshift-kube-apiserver",
                                    "severity": "info"
                                }
                            },
                            {
                                "alert": "PodSecurityViolation",
                                "annotations": {
                                    "description": "A workload (pod, deployment, daemonset, ...) was created in namespace \"{{ $labels.ocp_namespace }}\" but it did not match the PodSecurity \"{{ $labels.policy_level }}\" profile defined by its namespace either via the cluster-wide configuration (which triggers on a \"restricted\" profile violations) or by the namespace local Pod Security labels. Refer to Kubernetes documentation on Pod Security Admission to learn more about these violations.",
                                    "summary": "One or more workloads in platform namespaces of the cluster don't match their Pod Security profile"
                                },
                                "expr": "sum(increase(pod_security_evaluations_total{decision=\"deny\",mode=\"audit\",resource=\"pod\",ocp_namespace!=\"\"}[1d])) by (policy_level, ocp_namespace) \u003e 0\n",
                                "labels": {
                                    "namespace": "openshift-kube-apiserver",
                                    "severity": "info"
                                }
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "annotations": {
                    "include.release.openshift.io/ibm-cloud-managed": "true",
                    "include.release.openshift.io/self-managed-high-availability": "true",
                    "include.release.openshift.io/single-node-developer": "true"
                },
                "creationTimestamp": "2025-12-02T13:23:17Z",
                "generation": 1,
                "labels": {
                    "k8s-app": "machine-config-controller"
                },
                "name": "machine-config-controller",
                "namespace": "openshift-machine-config-operator",
                "ownerReferences": [
                    {
                        "apiVersion": "config.openshift.io/v1",
                        "controller": true,
                        "kind": "ClusterVersion",
                        "name": "version",
                        "uid": "46b5b63c-dc72-48b6-9f35-f2dea4314c26"
                    }
                ],
                "resourceVersion": "1220",
                "uid": "2f6c47d5-4322-48f1-9467-98d7120f19e2"
            },
            "spec": {
                "groups": [
                    {
                        "name": "os-image-override.rules",
                        "rules": [
                            {
                                "expr": "sum(os_image_url_override)",
                                "record": "os_image_url_override:sum"
                            }
                        ]
                    },
                    {
                        "name": "mcc-drain-error",
                        "rules": [
                            {
                                "alert": "MCCDrainError",
                                "annotations": {
                                    "description": "Drain failed on {{ $labels.exported_node }} , updates may be blocked. For more details check MachineConfigController pod logs: oc logs -f -n {{ $labels.namespace }} machine-config-controller-xxxxx -c machine-config-controller",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/machine-config-operator/MachineConfigControllerDrainError.md",
                                    "summary": "Alerts the user to a failed node drain. Always triggers when the failure happens one or more times."
                                },
                                "expr": "mcc_drain_err \u003e 0\n",
                                "labels": {
                                    "namespace": "openshift-machine-config-operator",
                                    "severity": "warning"
                                }
                            }
                        ]
                    },
                    {
                        "name": "mcc-pool-alert",
                        "rules": [
                            {
                                "alert": "MCCPoolAlert",
                                "annotations": {
                                    "description": "Node {{ $labels.exported_node }} has triggered a pool alert due to a label change. For more details check MachineConfigController pod logs: oc logs -f -n {{ $labels.namespace }} machine-config-controller-xxxxx -c machine-config-controller",
                                    "summary": "Triggers when nodes in a pool have overlapping labels such as master, worker, and a custom label therefore a choice must be made as to which is honored."
                                },
                                "expr": "mcc_pool_alert \u003e 0\n",
                                "labels": {
                                    "namespace": "openshift-machine-config-operator",
                                    "severity": "warning"
                                }
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "annotations": {
                    "include.release.openshift.io/ibm-cloud-managed": "true",
                    "include.release.openshift.io/self-managed-high-availability": "true",
                    "include.release.openshift.io/single-node-developer": "true"
                },
                "creationTimestamp": "2025-12-02T13:23:18Z",
                "generation": 1,
                "labels": {
                    "k8s-app": "machine-config-daemon"
                },
                "name": "machine-config-daemon",
                "namespace": "openshift-machine-config-operator",
                "ownerReferences": [
                    {
                        "apiVersion": "config.openshift.io/v1",
                        "controller": true,
                        "kind": "ClusterVersion",
                        "name": "version",
                        "uid": "46b5b63c-dc72-48b6-9f35-f2dea4314c26"
                    }
                ],
                "resourceVersion": "1252",
                "uid": "c50d7f4b-3ffa-482c-8431-bb9d7a974589"
            },
            "spec": {
                "groups": [
                    {
                        "name": "mcd-reboot-error",
                        "rules": [
                            {
                                "alert": "MCDRebootError",
                                "annotations": {
                                    "description": "Reboot failed on {{ $labels.node }} , update may be blocked. For more details:  oc logs -f -n {{ $labels.namespace }} {{ $labels.pod }} -c machine-config-daemon ",
                                    "summary": "Alerts the user that a node failed to reboot one or more times over a span of 5 minutes."
                                },
                                "expr": "mcd_reboots_failed_total \u003e 0\n",
                                "for": "5m",
                                "labels": {
                                    "namespace": "openshift-machine-config-operator",
                                    "severity": "critical"
                                }
                            }
                        ]
                    },
                    {
                        "name": "mcd-pivot-error",
                        "rules": [
                            {
                                "alert": "MCDPivotError",
                                "annotations": {
                                    "description": "Error detected in pivot logs on {{ $labels.node }} , upgrade may be blocked. For more details:  oc logs -f -n {{ $labels.namespace }} {{ $labels.pod }} -c machine-config-daemon ",
                                    "summary": "Alerts the user when an error is detected upon pivot. This triggers if the pivot errors are above zero for 2 minutes."
                                },
                                "expr": "mcd_pivot_errors_total \u003e 0\n",
                                "for": "2m",
                                "labels": {
                                    "namespace": "openshift-machine-config-operator",
                                    "severity": "warning"
                                }
                            }
                        ]
                    },
                    {
                        "name": "mcd-kubelet-health-state-error",
                        "rules": [
                            {
                                "alert": "KubeletHealthState",
                                "annotations": {
                                    "description": "Kubelet health failure threshold reached",
                                    "summary": "This keeps track of Kubelet health failures, and tallys them. The warning is triggered if 2 or more failures occur."
                                },
                                "expr": "mcd_kubelet_state \u003e 2\n",
                                "labels": {
                                    "namespace": "openshift-machine-config-operator",
                                    "severity": "warning"
                                }
                            }
                        ]
                    },
                    {
                        "name": "system-memory-exceeds-reservation",
                        "rules": [
                            {
                                "alert": "SystemMemoryExceedsReservation",
                                "annotations": {
                                    "description": "System memory usage of {{ $value | humanize }} on {{ $labels.node }} exceeds 95% of the reservation. Reserved memory ensures system processes can function even when the node is fully allocated and protects against workload out of memory events impacting the proper functioning of the node. The default reservation is expected to be sufficient for most configurations and should be increased (https://docs.openshift.com/container-platform/latest/nodes/nodes/nodes-nodes-managing.html) when running nodes with high numbers of pods (either due to rate of change or at steady state).",
                                    "summary": "Alerts the user when, for 15 miutes, a specific node is using more memory than is reserved"
                                },
                                "expr": "sum by (node) (container_memory_rss{id=\"/system.slice\"}) \u003e ((sum by (node) (kube_node_status_capacity{resource=\"memory\"} - kube_node_status_allocatable{resource=\"memory\"})) * 0.95)\n",
                                "for": "15m",
                                "labels": {
                                    "namespace": "openshift-machine-config-operator",
                                    "severity": "warning"
                                }
                            }
                        ]
                    },
                    {
                        "name": "high-overall-control-plane-memory",
                        "rules": [
                            {
                                "alert": "HighOverallControlPlaneMemory",
                                "annotations": {
                                    "description": "Given three control plane nodes, the overall memory utilization may only be about 2/3 of all available capacity. This is because if a single control plane node fails, the kube-apiserver and etcd may be slow to respond. To fix this, increase memory of the control plane nodes.",
                                    "summary": "Memory utilization across all control plane nodes is high, and could impact responsiveness and stability."
                                },
                                "expr": "(\n  1\n  -\n  sum (\n    node_memory_MemFree_bytes\n    + node_memory_Buffers_bytes\n    + node_memory_Cached_bytes\n    AND on (instance)\n    label_replace( kube_node_role{role=\"master\"}, \"instance\", \"$1\", \"node\", \"(.+)\" )\n  ) / sum (\n    node_memory_MemTotal_bytes\n    AND on (instance)\n    label_replace( kube_node_role{role=\"master\"}, \"instance\", \"$1\", \"node\", \"(.+)\" )\n  )\n) * 100 \u003e 60\n",
                                "for": "1h",
                                "labels": {
                                    "namespace": "openshift-machine-config-operator",
                                    "severity": "warning"
                                }
                            }
                        ]
                    },
                    {
                        "name": "extremely-high-individual-control-plane-memory",
                        "rules": [
                            {
                                "alert": "ExtremelyHighIndividualControlPlaneMemory",
                                "annotations": {
                                    "description": "The memory utilization per instance within control plane nodes influence the stability, and responsiveness of the cluster. This can lead to cluster instability and slow responses from kube-apiserver or failing requests specially on etcd. Moreover, OOM kill is expected which negatively influences the pod scheduling. If this happens on container level, the descheduler will not be able to detect it, as it works on the pod level. To fix this, increase memory of the affected node of control plane nodes.",
                                    "summary": "Extreme memory utilization per node within control plane nodes is extremely high, and could impact responsiveness and stability."
                                },
                                "expr": "(\n  1\n  -\n  sum by (instance) (\n    node_memory_MemFree_bytes\n    + node_memory_Buffers_bytes\n    + node_memory_Cached_bytes\n    AND on (instance)\n    label_replace( kube_node_role{role=\"master\"}, \"instance\", \"$1\", \"node\", \"(.+)\" )\n  ) / sum by (instance) (\n    node_memory_MemTotal_bytes\n    AND on (instance)\n    label_replace( kube_node_role{role=\"master\"}, \"instance\", \"$1\", \"node\", \"(.+)\" )\n  )\n) * 100 \u003e 90\n",
                                "for": "45m",
                                "labels": {
                                    "namespace": "openshift-machine-config-operator",
                                    "severity": "critical"
                                }
                            }
                        ]
                    },
                    {
                        "name": "mcd-missing-mc",
                        "rules": [
                            {
                                "alert": "MissingMachineConfig",
                                "annotations": {
                                    "description": "Could not find config {{ $labels.mc }} in-cluster, this likely indicates the MachineConfigs in-cluster has changed during the install process.  If you are seeing this when installing the cluster, please compare the in-cluster rendered machineconfigs to /etc/mcs-machine-config-content.json",
                                    "summary": "This keeps track of Machine Config failures. Specifically a common failure on install when a rendered Machine Config is missing. Triggered when this error happens once."
                                },
                                "expr": "mcd_missing_mc \u003e 0\n",
                                "labels": {
                                    "namespace": "openshift-machine-config-operator",
                                    "severity": "warning"
                                }
                            }
                        ]
                    },
                    {
                        "name": "telemetry.rules",
                        "rules": [
                            {
                                "expr": "count(mcd_local_unsupported_packages \u003e 0)",
                                "record": "cluster:mcd_nodes_with_unsupported_packages:count"
                            },
                            {
                                "expr": "sum(mcd_local_unsupported_packages)",
                                "record": "cluster:mcd_total_unsupported_packages:sum"
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "annotations": {
                    "include.release.openshift.io/ibm-cloud-managed": "true",
                    "include.release.openshift.io/self-managed-high-availability": "true",
                    "include.release.openshift.io/single-node-developer": "true"
                },
                "creationTimestamp": "2025-12-02T13:23:16Z",
                "generation": 1,
                "labels": {
                    "k8s-app": "machine-config-operator"
                },
                "name": "machine-config-operator",
                "namespace": "openshift-machine-config-operator",
                "ownerReferences": [
                    {
                        "apiVersion": "config.openshift.io/v1",
                        "controller": true,
                        "kind": "ClusterVersion",
                        "name": "version",
                        "uid": "46b5b63c-dc72-48b6-9f35-f2dea4314c26"
                    }
                ],
                "resourceVersion": "1189",
                "uid": "b8e6234f-e644-4ab0-9589-305de7209cfa"
            },
            "spec": {
                "groups": [
                    {
                        "name": "drain-override-configmap-present",
                        "rules": [
                            {
                                "alert": "MCODrainOverrideConfigMapAlert",
                                "annotations": {
                                    "description": "Image Registry Drain Override configmap has been detected. Please use the Node Disruption Policy feature to control the cluster's drain behavior as the configmap method is currently deprecated and will be removed in a future release.",
                                    "summary": "Alerts the user to the presence of a drain override configmap that is being deprecated and removed in a future release."
                                },
                                "expr": "mco_image_registry_drain_override_exists \u003e 0\n",
                                "labels": {
                                    "namespace": "openshift-machine-config-operator",
                                    "severity": "warning"
                                }
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "annotations": {
                    "capability.openshift.io/name": "marketplace",
                    "include.release.openshift.io/hypershift": "true",
                    "include.release.openshift.io/ibm-cloud-managed": "true",
                    "include.release.openshift.io/self-managed-high-availability": "true",
                    "include.release.openshift.io/single-node-developer": "true"
                },
                "creationTimestamp": "2025-12-02T13:23:16Z",
                "generation": 1,
                "labels": {
                    "prometheus": "alert-rules",
                    "role": "alert-rules"
                },
                "name": "marketplace-alert-rules",
                "namespace": "openshift-marketplace",
                "ownerReferences": [
                    {
                        "apiVersion": "config.openshift.io/v1",
                        "controller": true,
                        "kind": "ClusterVersion",
                        "name": "version",
                        "uid": "46b5b63c-dc72-48b6-9f35-f2dea4314c26"
                    }
                ],
                "resourceVersion": "1190",
                "uid": "e9630873-1bbe-4855-8d1b-65826353d277"
            },
            "spec": {
                "groups": [
                    {
                        "name": "operator.marketplace.rules",
                        "rules": [
                            {
                                "alert": "OperatorHubSourceError",
                                "annotations": {
                                    "description": "Operators shipped via the {{ $labels.name }} source are not available for installation until the issue is fixed. Operators already installed from this source will not receive updates until issue is fixed. Inspect the status of the pod owned by {{ $labels.name }} source in the openshift-marketplace namespace (oc -n openshift-marketplace get pods -l olm.catalogSource={{ $labels.name }}) to diagnose and repair.",
                                    "summary": "The {{ $labels.name }} source is in non-ready state for more than 10 minutes."
                                },
                                "expr": "catalogsource_ready{exported_namespace=\"openshift-marketplace\"} == 0",
                                "for": "10m",
                                "labels": {
                                    "severity": "warning"
                                }
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "creationTimestamp": "2025-12-02T13:31:12Z",
                "generation": 1,
                "labels": {
                    "app.kubernetes.io/component": "alert-router",
                    "app.kubernetes.io/instance": "main",
                    "app.kubernetes.io/managed-by": "cluster-monitoring-operator",
                    "app.kubernetes.io/name": "alertmanager",
                    "app.kubernetes.io/part-of": "openshift-monitoring",
                    "app.kubernetes.io/version": "0.27.0",
                    "prometheus": "k8s",
                    "role": "alert-rules"
                },
                "name": "alertmanager-main-rules",
                "namespace": "openshift-monitoring",
                "resourceVersion": "11915",
                "uid": "379e336d-9209-4883-bf97-97f42ebde0cd"
            },
            "spec": {
                "groups": [
                    {
                        "name": "alertmanager.rules",
                        "rules": [
                            {
                                "alert": "AlertmanagerFailedReload",
                                "annotations": {
                                    "description": "Configuration has failed to load for {{ $labels.namespace }}/{{ $labels.pod}}.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/AlertmanagerFailedReload.md",
                                    "summary": "Reloading an Alertmanager configuration has failed."
                                },
                                "expr": "# Without max_over_time, failed scrapes could create false negatives, see\n# https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.\nmax_over_time(alertmanager_config_last_reload_successful{job=~\"alertmanager-main|alertmanager-user-workload\"}[5m]) == 0\n",
                                "for": "10m",
                                "labels": {
                                    "severity": "critical"
                                }
                            },
                            {
                                "alert": "AlertmanagerMembersInconsistent",
                                "annotations": {
                                    "description": "Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} has only found {{ $value }} members of the {{$labels.job}} cluster.",
                                    "summary": "A member of an Alertmanager cluster has not found all other cluster members."
                                },
                                "expr": "# Without max_over_time, failed scrapes could create false negatives, see\n# https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.\n  max_over_time(alertmanager_cluster_members{job=~\"alertmanager-main|alertmanager-user-workload\"}[5m])\n\u003c on (namespace,service) group_left\n  count by (namespace,service) (max_over_time(alertmanager_cluster_members{job=~\"alertmanager-main|alertmanager-user-workload\"}[5m]))\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "AlertmanagerFailedToSendAlerts",
                                "annotations": {
                                    "description": "Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} failed to send {{ $value | humanizePercentage }} of notifications to {{ $labels.integration }}.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/AlertmanagerFailedToSendAlerts.md",
                                    "summary": "An Alertmanager instance failed to send notifications."
                                },
                                "expr": "(\n  rate(alertmanager_notifications_failed_total{job=~\"alertmanager-main|alertmanager-user-workload\"}[5m])\n/\n  ignoring (reason) group_left rate(alertmanager_notifications_total{job=~\"alertmanager-main|alertmanager-user-workload\"}[5m])\n)\n\u003e 0.01\n",
                                "for": "5m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "AlertmanagerClusterFailedToSendAlerts",
                                "annotations": {
                                    "description": "The minimum notification failure rate to {{ $labels.integration }} sent from any instance in the {{$labels.job}} cluster is {{ $value | humanizePercentage }}.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/AlertmanagerClusterFailedToSendAlerts.md",
                                    "summary": "All Alertmanager instances in a cluster failed to send notifications to a critical integration."
                                },
                                "expr": "min by (namespace,service, integration) (\n  rate(alertmanager_notifications_failed_total{job=~\"alertmanager-main|alertmanager-user-workload\", integration=~`.*`}[5m])\n/\n  ignoring (reason) group_left rate(alertmanager_notifications_total{job=~\"alertmanager-main|alertmanager-user-workload\", integration=~`.*`}[5m])\n)\n\u003e 0.01\n",
                                "for": "5m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "AlertmanagerConfigInconsistent",
                                "annotations": {
                                    "description": "Alertmanager instances within the {{$labels.job}} cluster have different configurations.",
                                    "summary": "Alertmanager instances within the same cluster have different configurations."
                                },
                                "expr": "count by (namespace,service) (\n  count_values by (namespace,service) (\"config_hash\", alertmanager_config_hash{job=~\"alertmanager-main|alertmanager-user-workload\"})\n)\n!= 1\n",
                                "for": "20m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "AlertmanagerClusterDown",
                                "annotations": {
                                    "description": "{{ $value | humanizePercentage }} of Alertmanager instances within the {{$labels.job}} cluster have been up for less than half of the last 5m.",
                                    "summary": "Half or more of the Alertmanager instances within the same cluster are down."
                                },
                                "expr": "(\n  count by (namespace,service) (\n    avg_over_time(up{job=~\"alertmanager-main|alertmanager-user-workload\"}[5m]) \u003c 0.5\n  )\n/\n  count by (namespace,service) (\n    up{job=~\"alertmanager-main|alertmanager-user-workload\"}\n  )\n)\n\u003e= 0.5\n",
                                "for": "5m",
                                "labels": {
                                    "severity": "warning"
                                }
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "creationTimestamp": "2025-12-02T13:30:12Z",
                "generation": 1,
                "labels": {
                    "app.kubernetes.io/component": "operator",
                    "app.kubernetes.io/managed-by": "cluster-monitoring-operator",
                    "app.kubernetes.io/name": "cluster-monitoring-operator",
                    "app.kubernetes.io/part-of": "openshift-monitoring",
                    "prometheus": "k8s",
                    "role": "alert-rules"
                },
                "name": "cluster-monitoring-operator-prometheus-rules",
                "namespace": "openshift-monitoring",
                "resourceVersion": "10015",
                "uid": "0a88dc18-fc80-4f84-9596-fcd66bad1b99"
            },
            "spec": {
                "groups": [
                    {
                        "name": "openshift-general.rules",
                        "rules": [
                            {
                                "alert": "TargetDown",
                                "annotations": {
                                    "description": "{{ printf \"%.4g\" $value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace have been unreachable for more than 15 minutes. This may be a symptom of network connectivity issues, down nodes, or failures within these components. Assess the health of the infrastructure and nodes running these targets and then contact support.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/TargetDown.md",
                                    "summary": "Some targets were not reachable from the monitoring server for an extended period of time."
                                },
                                "expr": "100 * ((\n  1 - sum   by (job, namespace, service) (up and on(namespace, pod) kube_pod_info) /\n      count by (job, namespace, service) (up and on(namespace, pod) kube_pod_info)\n) or (\n  count by (job, namespace, service) (up == 0) /\n  count by (job, namespace, service) (up)\n)) \u003e 10\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            }
                        ]
                    },
                    {
                        "name": "openshift-kubernetes.rules",
                        "rules": [
                            {
                                "expr": "sum(rate(container_cpu_usage_seconds_total{container=\"\",pod!=\"\"}[5m])) BY (pod, namespace)",
                                "record": "pod:container_cpu_usage:sum"
                            },
                            {
                                "expr": "sum(container_fs_usage_bytes{pod!=\"\"}) BY (pod, namespace)",
                                "record": "pod:container_fs_usage_bytes:sum"
                            },
                            {
                                "expr": "sum(container_memory_usage_bytes{container!=\"\"}) BY (namespace)",
                                "record": "namespace:container_memory_usage_bytes:sum"
                            },
                            {
                                "expr": "sum(rate(container_cpu_usage_seconds_total{container!=\"POD\",container!=\"\"}[5m])) BY (namespace)",
                                "record": "namespace:container_cpu_usage:sum"
                            },
                            {
                                "expr": "sum(container_memory_usage_bytes{container=\"\",pod!=\"\"}) BY (cluster) / sum(machine_memory_bytes) BY (cluster)",
                                "record": "cluster:memory_usage:ratio"
                            },
                            {
                                "expr": "sum(container_spec_cpu_shares{container=\"\",pod!=\"\"}) / 1000 / sum(machine_cpu_cores)",
                                "record": "cluster:container_spec_cpu_shares:ratio"
                            },
                            {
                                "expr": "sum(rate(container_cpu_usage_seconds_total{container=\"\",pod!=\"\"}[5m])) / sum(machine_cpu_cores)",
                                "record": "cluster:container_cpu_usage:ratio"
                            },
                            {
                                "expr": "sum by(namespace,pod, interface) (irate(container_network_receive_bytes_total{pod!=\"\"}[5m]))\n+\non(namespace,pod, interface) group_left(network_name) topk by(namespace,pod, interface) (1, pod_network_name_info)\n",
                                "record": "pod_interface_network:container_network_receive_bytes:irate5m"
                            },
                            {
                                "expr": "sum by(namespace,pod, interface) (irate(container_network_transmit_bytes_total{pod!=\"\"}[5m]))\n+\non(namespace,pod, interface) group_left(network_name) topk by(namespace,pod, interface) (1, pod_network_name_info)\n",
                                "record": "pod_interface_network:container_network_transmit_bytes_total:irate5m"
                            },
                            {
                                "expr": "max without(endpoint, instance, job, pod, service) (kube_node_labels and on(node) kube_node_role{role=\"master\"})",
                                "labels": {
                                    "label_node_role_kubernetes_io": "master",
                                    "label_node_role_kubernetes_io_master": "true"
                                },
                                "record": "cluster:master_nodes"
                            },
                            {
                                "expr": "max without(endpoint, instance, job, pod, service) (kube_node_labels and on(node) kube_node_role{role=\"infra\"})",
                                "labels": {
                                    "label_node_role_kubernetes_io_infra": "true"
                                },
                                "record": "cluster:infra_nodes"
                            },
                            {
                                "expr": "max without(endpoint, instance, job, pod, service) (cluster:master_nodes and on(node) cluster:infra_nodes)",
                                "labels": {
                                    "label_node_role_kubernetes_io_infra": "true",
                                    "label_node_role_kubernetes_io_master": "true"
                                },
                                "record": "cluster:master_infra_nodes"
                            },
                            {
                                "expr": "cluster:master_infra_nodes or on (node) cluster:master_nodes or on (node) cluster:infra_nodes or on (node) max without(endpoint, instance, job, pod, service) (kube_node_labels)",
                                "record": "cluster:nodes_roles"
                            },
                            {
                                "expr": "kube_node_labels and on(node) (sum(label_replace(node_cpu_info, \"node\", \"$1\", \"instance\", \"(.*)\")) by (node, package, core) == 2)",
                                "labels": {
                                    "label_node_hyperthread_enabled": "true"
                                },
                                "record": "cluster:hyperthread_enabled_nodes"
                            },
                            {
                                "expr": "count(sum(virt_platform) by (instance, type, system_manufacturer, system_product_name, baseboard_manufacturer, baseboard_product_name)) by (type, system_manufacturer, system_product_name, baseboard_manufacturer, baseboard_product_name)",
                                "record": "cluster:virt_platform_nodes:sum"
                            },
                            {
                                "expr": "sum by(label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io, label_kubernetes_io_arch, label_node_openshift_io_os_id) (\n  (\n    cluster:master_nodes\n    * on(node) group_left() max by(node)\n    (\n      kube_node_status_capacity{resource=\"cpu\",unit=\"core\"}\n    )\n  )\n  or on(node) (\n    label_replace(cluster:infra_nodes, \"label_node_role_kubernetes_io\", \"infra\", \"\", \"\")\n    * on(node) group_left() max by(node)\n    (\n      kube_node_status_capacity{resource=\"cpu\",unit=\"core\"}\n    )\n  )\n  or on(node) (\n    max without(endpoint, instance, job, pod, service)\n    (\n      kube_node_labels\n    ) * on(node) group_left() max by(node)\n    (\n      kube_node_status_capacity{resource=\"cpu\",unit=\"core\"}\n    )\n  )\n)\n",
                                "record": "cluster:capacity_cpu_cores:sum"
                            },
                            {
                                "expr": "clamp_max(\n  label_replace(\n    sum by(instance, package, core) (\n      node_cpu_info{core!=\"\",package!=\"\"}\n      or\n      # Assume core = cpu and package = 0 for platforms that don't expose core/package labels.\n      label_replace(label_join(node_cpu_info{core=\"\",package=\"\"}, \"core\", \"\", \"cpu\"), \"package\", \"0\", \"package\", \"\")\n    ) \u003e 1,\n    \"label_node_hyperthread_enabled\",\n    \"true\",\n    \"instance\",\n    \"(.*)\"\n  ) or on (instance, package)\n  label_replace(\n    sum by(instance, package, core) (\n      label_replace(node_cpu_info{core!=\"\",package!=\"\"}\n      or\n      # Assume core = cpu and package = 0 for platforms that don't expose core/package labels.\n      label_join(node_cpu_info{core=\"\",package=\"\"}, \"core\", \"\", \"cpu\"), \"package\", \"0\", \"package\", \"\")\n    ) \u003c= 1,\n    \"label_node_hyperthread_enabled\",\n    \"false\",\n    \"instance\",\n    \"(.*)\"\n  ),\n  1\n)\n",
                                "record": "cluster:cpu_core_hyperthreading"
                            },
                            {
                                "expr": "topk by(node) (1, cluster:nodes_roles) * on (node)\n  group_right( label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io, label_node_openshift_io_os_id, label_kubernetes_io_arch,\n               label_node_role_kubernetes_io_master, label_node_role_kubernetes_io_infra)\nlabel_replace( cluster:cpu_core_hyperthreading, \"node\", \"$1\", \"instance\", \"(.*)\" )\n",
                                "record": "cluster:cpu_core_node_labels"
                            },
                            {
                                "expr": "count(cluster:cpu_core_node_labels) by (label_beta_kubernetes_io_instance_type, label_node_hyperthread_enabled)",
                                "record": "cluster:capacity_cpu_cores_hyperthread_enabled:sum"
                            },
                            {
                                "expr": "sum by(label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io)\n(\n  (\n    cluster:master_nodes\n    * on(node) group_left() max by(node)\n    (\n      kube_node_status_capacity{resource=\"memory\",unit=\"byte\"}\n    )\n  )\n  or on(node)\n  (\n    max without(endpoint, instance, job, pod, service)\n    (\n      kube_node_labels\n    )\n    * on(node) group_left() max by(node)\n    (\n      kube_node_status_capacity{resource=\"memory\",unit=\"byte\"}\n    )\n  )\n)\n",
                                "record": "cluster:capacity_memory_bytes:sum"
                            },
                            {
                                "expr": "sum(1 - rate(node_cpu_seconds_total{mode=\"idle\"}[2m]) * on(namespace, pod) group_left(node) node_namespace_pod:kube_pod_info:{pod=~\"node-exporter.+\"})",
                                "record": "cluster:cpu_usage_cores:sum"
                            },
                            {
                                "expr": "sum(node_memory_MemTotal_bytes{job=\"node-exporter\"} - node_memory_MemAvailable_bytes{job=\"node-exporter\"})",
                                "record": "cluster:memory_usage_bytes:sum"
                            },
                            {
                                "expr": "sum(rate(container_cpu_usage_seconds_total{namespace!~\"openshift-.+\",pod!=\"\",container=\"\"}[5m]))",
                                "record": "workload:cpu_usage_cores:sum"
                            },
                            {
                                "expr": "cluster:cpu_usage_cores:sum - workload:cpu_usage_cores:sum",
                                "record": "openshift:cpu_usage_cores:sum"
                            },
                            {
                                "expr": "sum(container_memory_working_set_bytes{namespace!~\"openshift-.+\",pod!=\"\",container=\"\"})",
                                "record": "workload:memory_usage_bytes:sum"
                            },
                            {
                                "expr": "cluster:memory_usage_bytes:sum - workload:memory_usage_bytes:sum",
                                "record": "openshift:memory_usage_bytes:sum"
                            },
                            {
                                "expr": "sum(cluster:master_nodes or on(node) kube_node_labels ) BY (label_beta_kubernetes_io_instance_type, label_node_role_kubernetes_io, label_kubernetes_io_arch, label_node_openshift_io_os_id)",
                                "record": "cluster:node_instance_type_count:sum"
                            },
                            {
                                "expr": "sum by(provisioner) (\n  topk by (namespace, persistentvolumeclaim) (\n    1, kube_persistentvolumeclaim_resource_requests_storage_bytes\n  ) * on(namespace, persistentvolumeclaim) group_right()\n  topk by(namespace, persistentvolumeclaim) (\n    1, kube_persistentvolumeclaim_info * on(storageclass) group_left(provisioner) topk by(storageclass) (1, max by(storageclass, provisioner) (kube_storageclass_info))\n  )\n)\n",
                                "record": "cluster:kube_persistentvolumeclaim_resource_requests_storage_bytes:provisioner:sum"
                            },
                            {
                                "expr": "(sum(node_role_os_version_machine:cpu_capacity_cores:sum{label_node_role_kubernetes_io_master=\"\",label_node_role_kubernetes_io_infra=\"\"} or absent(__does_not_exist__)*0)) + ((sum(node_role_os_version_machine:cpu_capacity_cores:sum{label_node_role_kubernetes_io_master=\"true\"} or absent(__does_not_exist__)*0) * ((max(cluster_master_schedulable == 1)*0+1) or (absent(cluster_master_schedulable == 1)*0))))",
                                "record": "workload:capacity_physical_cpu_cores:sum"
                            },
                            {
                                "expr": "min_over_time(workload:capacity_physical_cpu_cores:sum[5m:15s])",
                                "record": "cluster:usage:workload:capacity_physical_cpu_cores:min:5m"
                            },
                            {
                                "expr": "max_over_time(workload:capacity_physical_cpu_cores:sum[5m:15s])",
                                "record": "cluster:usage:workload:capacity_physical_cpu_cores:max:5m"
                            },
                            {
                                "expr": "sum  by (provisioner) (\n  topk by (namespace, persistentvolumeclaim) (\n    1, kubelet_volume_stats_used_bytes\n  ) * on (namespace,persistentvolumeclaim) group_right()\n  topk by (namespace, persistentvolumeclaim) (\n    1, kube_persistentvolumeclaim_info * on(storageclass) group_left(provisioner) topk by(storageclass) (1, max by(storageclass, provisioner) (kube_storageclass_info))\n  )\n)\n",
                                "record": "cluster:kubelet_volume_stats_used_bytes:provisioner:sum"
                            },
                            {
                                "expr": "sum by (instance) (apiserver_storage_objects != -1)",
                                "record": "instance:etcd_object_counts:sum"
                            },
                            {
                                "expr": "topk(500, max by(resource) (apiserver_storage_objects != -1))",
                                "record": "cluster:usage:resources:sum"
                            },
                            {
                                "expr": "count(count (kube_pod_restart_policy{type!=\"Always\",namespace!~\"openshift-.+\"}) by (namespace,pod))",
                                "record": "cluster:usage:pods:terminal:workload:sum"
                            },
                            {
                                "expr": "sum(max(kubelet_containers_per_pod_count_sum) by (instance))",
                                "record": "cluster:usage:containers:sum"
                            },
                            {
                                "expr": "count(cluster:cpu_core_node_labels) by (label_kubernetes_io_arch, label_node_hyperthread_enabled, label_node_openshift_io_os_id,label_node_role_kubernetes_io_master,label_node_role_kubernetes_io_infra)",
                                "record": "node_role_os_version_machine:cpu_capacity_cores:sum"
                            },
                            {
                                "expr": "count(max(cluster:cpu_core_node_labels) by (node, package, label_beta_kubernetes_io_instance_type, label_node_hyperthread_enabled, label_node_role_kubernetes_io) ) by ( label_beta_kubernetes_io_instance_type, label_node_hyperthread_enabled, label_node_role_kubernetes_io)",
                                "record": "cluster:capacity_cpu_sockets_hyperthread_enabled:sum"
                            },
                            {
                                "expr": "count (max(cluster:cpu_core_node_labels) by (node, package, label_kubernetes_io_arch, label_node_hyperthread_enabled, label_node_openshift_io_os_id,label_node_role_kubernetes_io_master,label_node_role_kubernetes_io_infra) ) by (label_kubernetes_io_arch, label_node_hyperthread_enabled, label_node_openshift_io_os_id,label_node_role_kubernetes_io_master,label_node_role_kubernetes_io_infra)",
                                "record": "node_role_os_version_machine:cpu_capacity_sockets:sum"
                            },
                            {
                                "expr": "max(alertmanager_integrations{namespace=\"openshift-monitoring\"})",
                                "record": "cluster:alertmanager_integrations:max"
                            },
                            {
                                "expr": "sum by(plugin_name, volume_mode)(pv_collector_total_pv_count{volume_plugin!~\".*-e2e-.*\"})",
                                "record": "cluster:kube_persistentvolume_plugin_type_counts:sum"
                            },
                            {
                                "expr": "sum(\n  min by (node) (kube_node_status_condition{condition=\"Ready\",status=\"true\"})\n    and\n  max by (node) (kube_node_role{role=\"master\"})\n) == bool sum(kube_node_role{role=\"master\"})\n",
                                "record": "cluster:control_plane:all_nodes_ready"
                            },
                            {
                                "expr": "max by (profile) (cluster_monitoring_operator_collection_profile == 1)",
                                "record": "profile:cluster_monitoring_operator_collection_profile:max"
                            },
                            {
                                "alert": "ClusterMonitoringOperatorReconciliationErrors",
                                "annotations": {
                                    "description": "Errors are occurring during reconciliation cycles. Inspect the cluster-monitoring-operator log for potential root causes.",
                                    "summary": "Cluster Monitoring Operator is experiencing unexpected reconciliation errors."
                                },
                                "expr": "max_over_time(cluster_monitoring_operator_last_reconciliation_successful[5m]) == 0",
                                "for": "1h",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "ClusterMonitoringOperatorDeprecatedConfig",
                                "annotations": {
                                    "description": "The configuration field {{ $labels.field }} in {{ $labels.configmap }} was deprecated in {{ $labels.deprecation_version }} and has no effect.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/ClusterMonitoringOperatorDeprecatedConfig.md",
                                    "summary": "Cluster Monitoring Operator is being used with deprecated configuration."
                                },
                                "expr": "max by (configmap, field, deprecation_version) (cluster_monitoring_operator_deprecated_config_in_use) == 1",
                                "for": "1h",
                                "labels": {
                                    "severity": "info"
                                }
                            },
                            {
                                "alert": "AlertmanagerReceiversNotConfigured",
                                "annotations": {
                                    "description": "Alerts are not configured to be sent to a notification system, meaning that you may not be notified in a timely fashion when important failures occur. Check the OpenShift documentation to learn how to configure notifications with Alertmanager.",
                                    "summary": "Receivers (notification integrations) are not configured on Alertmanager"
                                },
                                "expr": "cluster:alertmanager_integrations:max == 0",
                                "for": "10m",
                                "labels": {
                                    "namespace": "openshift-monitoring",
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubeDeploymentReplicasMismatch",
                                "annotations": {
                                    "description": "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer than 15 minutes. This indicates that cluster infrastructure is unable to start or restart the necessary components. This most often occurs when one or more nodes are down or partioned from the cluster, or a fault occurs on the node that prevents the workload from starting. In rare cases this may indicate a new version of a cluster component cannot start due to a bug or configuration error. Assess the pods for this deployment to verify they are running on healthy nodes and then contact support.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeDeploymentReplicasMismatch.md",
                                    "summary": "Deployment has not matched the expected number of replicas"
                                },
                                "expr": "(((\n  kube_deployment_spec_replicas{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n    \u003e\n  kube_deployment_status_replicas_available{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n) and (\n  changes(kube_deployment_status_replicas_updated{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}[5m])\n    ==\n  0\n)) * on() group_left cluster:control_plane:all_nodes_ready) \u003e 0\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "expr": "avg_over_time((((count((max by (node) (up{job=\"kubelet\",metrics_path=\"/metrics\"} == 1) and max by (node) (kube_node_status_condition{condition=\"Ready\",status=\"true\"} == 1) and min by (node) (kube_node_spec_unschedulable == 0))) / scalar(count(min by (node) (kube_node_spec_unschedulable == 0))))))[5m:1s])",
                                "record": "cluster:usage:kube_schedulable_node_ready_reachable:avg5m"
                            },
                            {
                                "expr": "avg_over_time((count(max by (node) (kube_node_status_condition{condition=\"Ready\",status=\"true\"} == 1)) / scalar(count(max by (node) (kube_node_status_condition{condition=\"Ready\",status=\"true\"}))))[5m:1s])",
                                "record": "cluster:usage:kube_node_ready:avg5m"
                            },
                            {
                                "expr": "(max without (condition,container,endpoint,instance,job,service) (((kube_pod_status_ready{condition=\"false\"} == 1)*0 or (kube_pod_status_ready{condition=\"true\"} == 1)) * on(pod,namespace) group_left() group by (pod,namespace) (kube_pod_status_phase{phase=~\"Running|Unknown|Pending\"} == 1)))",
                                "record": "kube_running_pod_ready"
                            },
                            {
                                "expr": "avg(kube_running_pod_ready{namespace=~\"openshift-.*\"})",
                                "record": "cluster:usage:openshift:kube_running_pod_ready:avg"
                            },
                            {
                                "expr": "avg(kube_running_pod_ready{namespace!~\"openshift-.*\"})",
                                "record": "cluster:usage:workload:kube_running_pod_ready:avg"
                            },
                            {
                                "alert": "KubePodNotScheduled",
                                "annotations": {
                                    "description": "Pod {{ $labels.namespace }}/{{ $labels.pod }} cannot be scheduled for more than 30 minutes.\nCheck the details of the pod with the following command:\noc describe -n {{ $labels.namespace }} pod {{ $labels.pod }}",
                                    "summary": "Pod cannot be scheduled."
                                },
                                "expr": "last_over_time(kube_pod_status_unschedulable{namespace=~\"(openshift-.*|kube-.*|default)\"}[5m]) == 1",
                                "for": "30m",
                                "labels": {
                                    "severity": "warning"
                                }
                            }
                        ]
                    },
                    {
                        "interval": "30s",
                        "name": "kubernetes-recurring.rules",
                        "rules": [
                            {
                                "expr": "sum_over_time(workload:capacity_physical_cpu_cores:sum[30s:1s]) + ((cluster:usage:workload:capacity_physical_cpu_core_seconds offset 25s) or (absent(cluster:usage:workload:capacity_physical_cpu_core_seconds offset 25s)*0))",
                                "record": "cluster:usage:workload:capacity_physical_cpu_core_seconds"
                            }
                        ]
                    },
                    {
                        "name": "openshift-ingress.rules",
                        "rules": [
                            {
                                "expr": "sum by (code) (rate(haproxy_server_http_responses_total[5m]) \u003e 0)",
                                "record": "code:cluster:ingress_http_request_count:rate5m:sum"
                            },
                            {
                                "expr": "sum (rate(haproxy_frontend_bytes_in_total[5m]))",
                                "record": "cluster:usage:ingress_frontend_bytes_in:rate5m:sum"
                            },
                            {
                                "expr": "sum (rate(haproxy_frontend_bytes_out_total[5m]))",
                                "record": "cluster:usage:ingress_frontend_bytes_out:rate5m:sum"
                            },
                            {
                                "expr": "sum (haproxy_frontend_current_sessions)",
                                "record": "cluster:usage:ingress_frontend_connections:sum"
                            },
                            {
                                "expr": "sum(max without(service,endpoint,container,pod,job,namespace) (increase(haproxy_server_http_responses_total{code!~\"2xx|1xx|4xx|3xx\",exported_namespace!~\"openshift-.*\"}[5m]) \u003e 0)) / sum (max without(service,endpoint,container,pod,job,namespace) (increase(haproxy_server_http_responses_total{exported_namespace!~\"openshift-.*\"}[5m]))) or absent(__does_not_exist__)*0",
                                "record": "cluster:usage:workload:ingress_request_error:fraction5m"
                            },
                            {
                                "expr": "sum (max without(service,endpoint,container,pod,job,namespace) (irate(haproxy_server_http_responses_total{exported_namespace!~\"openshift-.*\"}[5m]))) or absent(__does_not_exist__)*0",
                                "record": "cluster:usage:workload:ingress_request_total:irate5m"
                            },
                            {
                                "expr": "sum(max without(service,endpoint,container,pod,job,namespace) (increase(haproxy_server_http_responses_total{code!~\"2xx|1xx|4xx|3xx\",exported_namespace=~\"openshift-.*\"}[5m]) \u003e 0)) / sum (max without(service,endpoint,container,pod,job,namespace) (increase(haproxy_server_http_responses_total{exported_namespace=~\"openshift-.*\"}[5m]))) or absent(__does_not_exist__)*0",
                                "record": "cluster:usage:openshift:ingress_request_error:fraction5m"
                            },
                            {
                                "expr": "sum (max without(service,endpoint,container,pod,job,namespace) (irate(haproxy_server_http_responses_total{exported_namespace=~\"openshift-.*\"}[5m]))) or absent(__does_not_exist__)*0",
                                "record": "cluster:usage:openshift:ingress_request_total:irate5m"
                            },
                            {
                                "expr": "sum(ingress_controller_aws_nlb_active) or vector(0)",
                                "record": "cluster:ingress_controller_aws_nlb_active:sum"
                            }
                        ]
                    },
                    {
                        "name": "openshift-build.rules",
                        "rules": [
                            {
                                "expr": "sum by (strategy) (openshift_build_status_phase_total)",
                                "record": "openshift:build_by_strategy:sum"
                            }
                        ]
                    },
                    {
                        "name": "openshift-monitoring.rules",
                        "rules": [
                            {
                                "expr": "sum by (job,namespace) (max without(instance) (prometheus_tsdb_head_series{namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}))",
                                "record": "openshift:prometheus_tsdb_head_series:sum"
                            },
                            {
                                "expr": "sum by(job,namespace) (max without(instance) (rate(prometheus_tsdb_head_samples_appended_total{namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[2m])))",
                                "record": "openshift:prometheus_tsdb_head_samples_appended_total:sum"
                            },
                            {
                                "expr": "sum by (namespace) (max without(instance) (container_memory_working_set_bytes{namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\", container=\"\"}))",
                                "record": "monitoring:container_memory_working_set_bytes:sum"
                            },
                            {
                                "expr": "topk(3, sum by(namespace, job)(sum_over_time(scrape_series_added[1h])))",
                                "record": "namespace_job:scrape_series_added:topk3_sum1h"
                            },
                            {
                                "expr": "topk(3, max by(namespace, job) (topk by(namespace,job) (1, scrape_samples_post_metric_relabeling)))",
                                "record": "namespace_job:scrape_samples_post_metric_relabeling:topk3"
                            },
                            {
                                "expr": "sum by(exported_service) (rate(haproxy_server_http_responses_total{exported_namespace=\"openshift-monitoring\", exported_service=~\"alertmanager-main|prometheus-k8s\"}[5m]))",
                                "record": "monitoring:haproxy_server_http_responses_total:sum"
                            },
                            {
                                "expr": "max by (cluster, namespace, workload, pod) (label_replace(label_replace(kube_pod_owner{job=\"kube-state-metrics\", owner_kind=\"ReplicationController\"},\"replicationcontroller\", \"$1\", \"owner_name\", \"(.*)\") * on(replicationcontroller, namespace) group_left(owner_name) topk by(replicationcontroller, namespace) (1, max by (replicationcontroller, namespace, owner_name) (kube_replicationcontroller_owner{job=\"kube-state-metrics\"})),\"workload\", \"$1\", \"owner_name\", \"(.*)\"))",
                                "labels": {
                                    "workload_type": "deploymentconfig"
                                },
                                "record": "namespace_workload_pod:kube_pod_owner:relabel"
                            }
                        ]
                    },
                    {
                        "name": "openshift-etcd-telemetry.rules",
                        "rules": [
                            {
                                "expr": "sum by (instance) (etcd_mvcc_db_total_size_in_bytes{job=\"etcd\"})",
                                "record": "instance:etcd_mvcc_db_total_size_in_bytes:sum"
                            },
                            {
                                "expr": "histogram_quantile(0.99, sum by (instance, le) (rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=\"etcd\"}[5m])))",
                                "labels": {
                                    "quantile": "0.99"
                                },
                                "record": "instance:etcd_disk_wal_fsync_duration_seconds:histogram_quantile"
                            },
                            {
                                "expr": "histogram_quantile(0.99, sum by (instance, le) (rate(etcd_network_peer_round_trip_time_seconds_bucket{job=\"etcd\"}[5m])))",
                                "labels": {
                                    "quantile": "0.99"
                                },
                                "record": "instance:etcd_network_peer_round_trip_time_seconds:histogram_quantile"
                            },
                            {
                                "expr": "sum by (instance) (etcd_mvcc_db_total_size_in_use_in_bytes{job=\"etcd\"})",
                                "record": "instance:etcd_mvcc_db_total_size_in_use_in_bytes:sum"
                            },
                            {
                                "expr": "histogram_quantile(0.99, sum by (instance, le) (rate(etcd_disk_backend_commit_duration_seconds_bucket{job=\"etcd\"}[5m])))",
                                "labels": {
                                    "quantile": "0.99"
                                },
                                "record": "instance:etcd_disk_backend_commit_duration_seconds:histogram_quantile"
                            }
                        ]
                    },
                    {
                        "name": "openshift-sre.rules",
                        "rules": [
                            {
                                "expr": "sum(rate(apiserver_request_total{job=\"apiserver\"}[10m])) BY (code)",
                                "record": "code:apiserver_request_total:rate:sum"
                            }
                        ]
                    },
                    {
                        "name": "apiserver-list-watch.rules",
                        "rules": [
                            {
                                "expr": "sum by(verb) (rate(apiserver_request_total{verb=~\"LIST|WATCH\",code=~\"2..\"}[5m]))",
                                "record": "apiserver_list_watch_request_success_total:rate:sum"
                            }
                        ]
                    },
                    {
                        "name": "general.rules",
                        "rules": [
                            {
                                "alert": "Watchdog",
                                "annotations": {
                                    "description": "This is an alert meant to ensure that the entire alerting pipeline is functional.\nThis alert is always firing, therefore it should always be firing in Alertmanager\nand always fire against a receiver. There are integrations with various notification\nmechanisms that send a notification when this alert is not firing. For example the\n\"DeadMansSnitch\" integration in PagerDuty.\n",
                                    "summary": "An alert that should always be firing to certify that Alertmanager is working properly."
                                },
                                "expr": "vector(1)",
                                "labels": {
                                    "namespace": "openshift-monitoring",
                                    "severity": "none"
                                }
                            }
                        ]
                    },
                    {
                        "name": "node-network",
                        "rules": [
                            {
                                "alert": "NodeNetworkInterfaceFlapping",
                                "annotations": {
                                    "description": "Network interface \"{{ $labels.device }}\" changing its up status often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}",
                                    "summary": "Network interface is often changing its status"
                                },
                                "expr": "changes(node_network_up{job=\"node-exporter\",device!~\"veth.+|tunbr\"}[2m]) \u003e 2\n",
                                "for": "2m",
                                "labels": {
                                    "severity": "warning"
                                }
                            }
                        ]
                    },
                    {
                        "name": "kube-prometheus-node-recording.rules",
                        "rules": [
                            {
                                "expr": "sum(rate(node_cpu_seconds_total{mode!=\"idle\",mode!=\"iowait\",mode!=\"steal\"}[3m])) BY (instance)",
                                "record": "instance:node_cpu:rate:sum"
                            },
                            {
                                "expr": "sum(rate(node_network_receive_bytes_total[3m])) BY (instance)",
                                "record": "instance:node_network_receive_bytes:rate:sum"
                            },
                            {
                                "expr": "sum(rate(node_network_transmit_bytes_total[3m])) BY (instance)",
                                "record": "instance:node_network_transmit_bytes:rate:sum"
                            },
                            {
                                "expr": "sum(rate(node_cpu_seconds_total{mode!=\"idle\",mode!=\"iowait\",mode!=\"steal\"}[5m]))",
                                "record": "cluster:node_cpu:sum_rate5m"
                            },
                            {
                                "expr": "cluster:node_cpu:sum_rate5m / count(sum(node_cpu_seconds_total) BY (instance, cpu))",
                                "record": "cluster:node_cpu:ratio"
                            }
                        ]
                    },
                    {
                        "name": "kube-prometheus-general.rules",
                        "rules": [
                            {
                                "expr": "count without(instance, pod, node) (up == 1)",
                                "record": "count:up1"
                            },
                            {
                                "expr": "count without(instance, pod, node) (up == 0)",
                                "record": "count:up0"
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "creationTimestamp": "2025-12-02T13:30:18Z",
                "generation": 1,
                "labels": {
                    "app.kubernetes.io/component": "exporter",
                    "app.kubernetes.io/managed-by": "cluster-monitoring-operator",
                    "app.kubernetes.io/name": "kube-state-metrics",
                    "app.kubernetes.io/part-of": "openshift-monitoring",
                    "app.kubernetes.io/version": "2.13.0",
                    "prometheus": "k8s",
                    "role": "alert-rules"
                },
                "name": "kube-state-metrics-rules",
                "namespace": "openshift-monitoring",
                "resourceVersion": "10407",
                "uid": "9dd2b568-712f-4f19-a845-1cb8db15073c"
            },
            "spec": {
                "groups": [
                    {
                        "name": "kube-state-metrics",
                        "rules": [
                            {
                                "alert": "KubeStateMetricsListErrors",
                                "annotations": {
                                    "description": "kube-state-metrics is experiencing errors at an elevated rate in list operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.",
                                    "summary": "kube-state-metrics is experiencing errors in list operations."
                                },
                                "expr": "(sum(rate(kube_state_metrics_list_total{job=\"kube-state-metrics\",result=\"error\"}[5m])) by (cluster)\n  /\nsum(rate(kube_state_metrics_list_total{job=\"kube-state-metrics\"}[5m])) by (cluster))\n\u003e 0.01\n",
                                "for": "15m",
                                "labels": {
                                    "namespace": "openshift-monitoring",
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubeStateMetricsWatchErrors",
                                "annotations": {
                                    "description": "kube-state-metrics is experiencing errors at an elevated rate in watch operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.",
                                    "summary": "kube-state-metrics is experiencing errors in watch operations."
                                },
                                "expr": "(sum(rate(kube_state_metrics_watch_total{job=\"kube-state-metrics\",result=\"error\"}[5m])) by (cluster)\n  /\nsum(rate(kube_state_metrics_watch_total{job=\"kube-state-metrics\"}[5m])) by (cluster))\n\u003e 0.01\n",
                                "for": "15m",
                                "labels": {
                                    "namespace": "openshift-monitoring",
                                    "severity": "warning"
                                }
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "creationTimestamp": "2025-12-02T13:30:10Z",
                "generation": 1,
                "labels": {
                    "app.kubernetes.io/managed-by": "cluster-monitoring-operator",
                    "app.kubernetes.io/name": "kube-prometheus",
                    "app.kubernetes.io/part-of": "openshift-monitoring",
                    "prometheus": "k8s",
                    "role": "alert-rules"
                },
                "name": "kubernetes-monitoring-rules",
                "namespace": "openshift-monitoring",
                "resourceVersion": "9707",
                "uid": "0917cd10-df65-42e8-9f0e-142cb1676bd8"
            },
            "spec": {
                "groups": [
                    {
                        "name": "kubernetes-apps",
                        "rules": [
                            {
                                "alert": "KubePodCrashLooping",
                                "annotations": {
                                    "description": "Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is in waiting state (reason: \"CrashLoopBackOff\").",
                                    "summary": "Pod is crash looping."
                                },
                                "expr": "max_over_time(kube_pod_container_status_waiting_reason{reason=\"CrashLoopBackOff\", namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}[5m]) \u003e= 1\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubePodNotReady",
                                "annotations": {
                                    "description": "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than 15 minutes.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubePodNotReady.md",
                                    "summary": "Pod has been in a non-ready state for more than 15 minutes."
                                },
                                "expr": "sum by (namespace, pod, cluster) (\n  max by(namespace, pod, cluster) (\n    kube_pod_status_phase{namespace=~\"(openshift-.*|kube-.*|default)\", job=\"kube-state-metrics\", phase=~\"Pending|Unknown\"}\n    unless ignoring(phase) (kube_pod_status_unschedulable{job=\"kube-state-metrics\"} == 1)\n  ) * on(namespace, pod, cluster) group_left(owner_kind) topk by(namespace, pod, cluster) (\n    1, max by(namespace, pod, owner_kind, cluster) (kube_pod_owner{owner_kind!=\"Job\"})\n  )\n) \u003e 0\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubeDeploymentGenerationMismatch",
                                "annotations": {
                                    "description": "Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment }} does not match, this indicates that the Deployment has failed but has not been rolled back.",
                                    "summary": "Deployment generation mismatch due to possible roll-back"
                                },
                                "expr": "kube_deployment_status_observed_generation{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n  !=\nkube_deployment_metadata_generation{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubeDeploymentRolloutStuck",
                                "annotations": {
                                    "description": "Rollout of deployment {{ $labels.namespace }}/{{ $labels.deployment }} is not progressing for longer than 15 minutes.",
                                    "summary": "Deployment rollout is not progressing."
                                },
                                "expr": "kube_deployment_status_condition{condition=\"Progressing\", status=\"false\",namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n!= 0\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubeStatefulSetReplicasMismatch",
                                "annotations": {
                                    "description": "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not matched the expected number of replicas for longer than 15 minutes.",
                                    "summary": "StatefulSet has not matched the expected number of replicas."
                                },
                                "expr": "(\n  kube_statefulset_status_replicas_ready{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n    !=\n  kube_statefulset_status_replicas{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n) and (\n  changes(kube_statefulset_status_replicas_updated{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}[10m])\n    ==\n  0\n)\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubeStatefulSetGenerationMismatch",
                                "annotations": {
                                    "description": "StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset }} does not match, this indicates that the StatefulSet has failed but has not been rolled back.",
                                    "summary": "StatefulSet generation mismatch due to possible roll-back"
                                },
                                "expr": "kube_statefulset_status_observed_generation{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n  !=\nkube_statefulset_metadata_generation{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubeStatefulSetUpdateNotRolledOut",
                                "annotations": {
                                    "description": "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update has not been rolled out.",
                                    "summary": "StatefulSet update has not been rolled out."
                                },
                                "expr": "(\n  max without (revision) (\n    kube_statefulset_status_current_revision{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n      unless\n    kube_statefulset_status_update_revision{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n  )\n    *\n  (\n    kube_statefulset_replicas{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n      !=\n    kube_statefulset_status_replicas_updated{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n  )\n)  and (\n  changes(kube_statefulset_status_replicas_updated{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}[5m])\n    ==\n  0\n)\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubeDaemonSetRolloutStuck",
                                "annotations": {
                                    "description": "DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} has not finished or progressed for at least 30 minutes.",
                                    "summary": "DaemonSet rollout is stuck."
                                },
                                "expr": "(\n  (\n    kube_daemonset_status_current_number_scheduled{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n     !=\n    kube_daemonset_status_desired_number_scheduled{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n  ) or (\n    kube_daemonset_status_number_misscheduled{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n     !=\n    0\n  ) or (\n    kube_daemonset_status_updated_number_scheduled{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n     !=\n    kube_daemonset_status_desired_number_scheduled{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n  ) or (\n    kube_daemonset_status_number_available{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n     !=\n    kube_daemonset_status_desired_number_scheduled{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n  )\n) and (\n  changes(kube_daemonset_status_updated_number_scheduled{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}[5m])\n    ==\n  0\n)\n",
                                "for": "30m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubeContainerWaiting",
                                "annotations": {
                                    "description": "pod/{{ $labels.pod }} in namespace {{ $labels.namespace }} on container {{ $labels.container}} has been in waiting state for longer than 1 hour.",
                                    "summary": "Pod container waiting longer than 1 hour"
                                },
                                "expr": "sum by (namespace, pod, container, cluster) (kube_pod_container_status_waiting_reason{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}) \u003e 0\n",
                                "for": "1h",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubeDaemonSetNotScheduled",
                                "annotations": {
                                    "description": "{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled.",
                                    "summary": "DaemonSet pods are not scheduled."
                                },
                                "expr": "kube_daemonset_status_desired_number_scheduled{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n  -\nkube_daemonset_status_current_number_scheduled{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"} \u003e 0\n",
                                "for": "10m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubeDaemonSetMisScheduled",
                                "annotations": {
                                    "description": "{{ $value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are running where they are not supposed to run.",
                                    "summary": "DaemonSet pods are misscheduled."
                                },
                                "expr": "kube_daemonset_status_number_misscheduled{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"} \u003e 0\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubeJobNotCompleted",
                                "annotations": {
                                    "description": "Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more than {{ \"43200\" | humanizeDuration }} to complete.",
                                    "summary": "Job did not complete in time"
                                },
                                "expr": "time() - max by(namespace, job_name, cluster) (kube_job_status_start_time{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n  and\nkube_job_status_active{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"} \u003e 0) \u003e 43200\n",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubeJobFailed",
                                "annotations": {
                                    "description": "Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete. Removing failed job after investigation should clear this alert.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeJobFailed.md",
                                    "summary": "Job failed to complete."
                                },
                                "expr": "kube_job_failed{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}  \u003e 0\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubeHpaReplicasMismatch",
                                "annotations": {
                                    "description": "HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }} has not matched the desired number of replicas for longer than 15 minutes.",
                                    "summary": "HPA has not matched desired number of replicas."
                                },
                                "expr": "(kube_horizontalpodautoscaler_status_desired_replicas{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n  !=\nkube_horizontalpodautoscaler_status_current_replicas{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"})\n  and\n(kube_horizontalpodautoscaler_status_current_replicas{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n  \u003e\nkube_horizontalpodautoscaler_spec_min_replicas{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"})\n  and\n(kube_horizontalpodautoscaler_status_current_replicas{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n  \u003c\nkube_horizontalpodautoscaler_spec_max_replicas{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"})\n  and\nchanges(kube_horizontalpodautoscaler_status_current_replicas{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}[15m]) == 0\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubeHpaMaxedOut",
                                "annotations": {
                                    "description": "HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }} has been running at max replicas for longer than 15 minutes.",
                                    "summary": "HPA is running at max replicas"
                                },
                                "expr": "kube_horizontalpodautoscaler_status_current_replicas{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n  ==\nkube_horizontalpodautoscaler_spec_max_replicas{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\"}\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            }
                        ]
                    },
                    {
                        "name": "kubernetes-resources",
                        "rules": [
                            {
                                "alert": "KubeCPUOvercommit",
                                "annotations": {
                                    "description": "Cluster {{ $labels.cluster }} has overcommitted CPU resource requests for Pods by {{ $value }} CPU shares and cannot tolerate node failure.",
                                    "summary": "Cluster has overcommitted CPU resource requests."
                                },
                                "expr": "sum(namespace_cpu:kube_pod_container_resource_requests:sum{job=\"kube-state-metrics\",}) by (cluster) - (sum(kube_node_status_allocatable{job=\"kube-state-metrics\",resource=\"cpu\"}) by (cluster) - max(kube_node_status_allocatable{job=\"kube-state-metrics\",resource=\"cpu\"}) by (cluster)) \u003e 0\nand\n(sum(kube_node_status_allocatable{job=\"kube-state-metrics\",resource=\"cpu\"}) by (cluster) - max(kube_node_status_allocatable{job=\"kube-state-metrics\",resource=\"cpu\"}) by (cluster)) \u003e 0\n",
                                "for": "10m",
                                "labels": {
                                    "namespace": "kube-system",
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubeMemoryOvercommit",
                                "annotations": {
                                    "description": "Cluster {{ $labels.cluster }} has overcommitted memory resource requests for Pods by {{ $value | humanize }} bytes and cannot tolerate node failure.",
                                    "summary": "Cluster has overcommitted memory resource requests."
                                },
                                "expr": "sum(namespace_memory:kube_pod_container_resource_requests:sum{}) by (cluster) - (sum(kube_node_status_allocatable{resource=\"memory\", job=\"kube-state-metrics\"}) by (cluster) - max(kube_node_status_allocatable{resource=\"memory\", job=\"kube-state-metrics\"}) by (cluster)) \u003e 0\nand\n(sum(kube_node_status_allocatable{resource=\"memory\", job=\"kube-state-metrics\"}) by (cluster) - max(kube_node_status_allocatable{resource=\"memory\", job=\"kube-state-metrics\"}) by (cluster)) \u003e 0\n",
                                "for": "10m",
                                "labels": {
                                    "namespace": "kube-system",
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubeQuotaAlmostFull",
                                "annotations": {
                                    "description": "Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.",
                                    "summary": "Namespace quota is going to be full."
                                },
                                "expr": "kube_resourcequota{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\", type=\"used\"}\n  / ignoring(instance, job, type)\n(kube_resourcequota{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\", type=\"hard\"} \u003e 0)\n  \u003e 0.9 \u003c 1\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "info"
                                }
                            },
                            {
                                "alert": "KubeQuotaFullyUsed",
                                "annotations": {
                                    "description": "Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.",
                                    "summary": "Namespace quota is fully used."
                                },
                                "expr": "kube_resourcequota{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\", type=\"used\"}\n  / ignoring(instance, job, type)\n(kube_resourcequota{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\", type=\"hard\"} \u003e 0)\n  == 1\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "info"
                                }
                            },
                            {
                                "alert": "KubeQuotaExceeded",
                                "annotations": {
                                    "description": "Namespace {{ $labels.namespace }} is using {{ $value | humanizePercentage }} of its {{ $labels.resource }} quota.",
                                    "summary": "Namespace quota has exceeded the limits."
                                },
                                "expr": "kube_resourcequota{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\", type=\"used\"}\n  / ignoring(instance, job, type)\n(kube_resourcequota{namespace=~\"(openshift-.*|kube-.*|default)\",job=\"kube-state-metrics\", type=\"hard\"} \u003e 0)\n  \u003e 1\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            }
                        ]
                    },
                    {
                        "name": "kubernetes-system",
                        "rules": [
                            {
                                "alert": "KubeClientErrors",
                                "annotations": {
                                    "description": "Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ $value | humanizePercentage }} errors.'",
                                    "summary": "Kubernetes API server client is experiencing errors."
                                },
                                "expr": "(sum(rate(rest_client_requests_total{job=\"apiserver\",code=~\"5..\"}[5m])) by (cluster, instance, job, namespace)\n  /\nsum(rate(rest_client_requests_total{job=\"apiserver\"}[5m])) by (cluster, instance, job, namespace))\n\u003e 0.01\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            }
                        ]
                    },
                    {
                        "name": "kubernetes-system-kubelet",
                        "rules": [
                            {
                                "alert": "KubeNodeNotReady",
                                "annotations": {
                                    "description": "{{ $labels.node }} has been unready for more than 15 minutes.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeNodeNotReady.md",
                                    "summary": "Node is not ready."
                                },
                                "expr": "kube_node_status_condition{job=\"kube-state-metrics\",condition=\"Ready\",status=\"true\"} == 0\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubeNodeUnreachable",
                                "annotations": {
                                    "description": "{{ $labels.node }} is unreachable and some workloads may be rescheduled.",
                                    "summary": "Node is unreachable."
                                },
                                "expr": "(kube_node_spec_taint{job=\"kube-state-metrics\",key=\"node.kubernetes.io/unreachable\",effect=\"NoSchedule\"} unless ignoring(key,value) kube_node_spec_taint{job=\"kube-state-metrics\",key=~\"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn\"}) == 1\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubeletTooManyPods",
                                "annotations": {
                                    "description": "Kubelet '{{ $labels.node }}' is running at {{ $value | humanizePercentage }} of its Pod capacity.",
                                    "summary": "Kubelet is running at capacity."
                                },
                                "expr": "count by(cluster, node) (\n  (kube_pod_status_phase{job=\"kube-state-metrics\",phase=\"Running\"} == 1) * on(instance,pod,namespace,cluster) group_left(node) topk by(instance,pod,namespace,cluster) (1, kube_pod_info{job=\"kube-state-metrics\"})\n)\n/\nmax by(cluster, node) (\n  kube_node_status_capacity{job=\"kube-state-metrics\",resource=\"pods\"} != 1\n) \u003e 0.95\n",
                                "for": "15m",
                                "labels": {
                                    "namespace": "kube-system",
                                    "severity": "info"
                                }
                            },
                            {
                                "alert": "KubeNodeReadinessFlapping",
                                "annotations": {
                                    "description": "The readiness status of node {{ $labels.node }} has changed {{ $value }} times in the last 15 minutes.",
                                    "summary": "Node readiness status is flapping."
                                },
                                "expr": "sum(changes(kube_node_status_condition{job=\"kube-state-metrics\",status=\"true\",condition=\"Ready\"}[15m])) by (cluster, node) \u003e 2\n",
                                "for": "15m",
                                "labels": {
                                    "namespace": "kube-system",
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubeletPlegDurationHigh",
                                "annotations": {
                                    "description": "The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of {{ $value }} seconds on node {{ $labels.node }}.",
                                    "summary": "Kubelet Pod Lifecycle Event Generator is taking too long to relist."
                                },
                                "expr": "node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile=\"0.99\"} \u003e= 10\n",
                                "for": "5m",
                                "labels": {
                                    "namespace": "kube-system",
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubeletPodStartUpLatencyHigh",
                                "annotations": {
                                    "description": "Kubelet Pod startup 99th percentile latency is {{ $value }} seconds on node {{ $labels.node }}.",
                                    "summary": "Kubelet Pod startup latency is too high."
                                },
                                "expr": "histogram_quantile(0.99, sum(rate(kubelet_pod_worker_duration_seconds_bucket{job=\"kubelet\", metrics_path=\"/metrics\"}[5m])) by (cluster, instance, le)) * on(cluster, instance) group_left(node) kubelet_node_name{job=\"kubelet\", metrics_path=\"/metrics\"} \u003e 60\n",
                                "for": "15m",
                                "labels": {
                                    "namespace": "kube-system",
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubeletClientCertificateRenewalErrors",
                                "annotations": {
                                    "description": "Kubelet on node {{ $labels.node }} has failed to renew its client certificate ({{ $value | humanize }} errors in the last 5 minutes).",
                                    "summary": "Kubelet has failed to renew its client certificate."
                                },
                                "expr": "increase(kubelet_certificate_manager_client_expiration_renew_errors[5m]) \u003e 0\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubeletServerCertificateRenewalErrors",
                                "annotations": {
                                    "description": "Kubelet on node {{ $labels.node }} has failed to renew its server certificate ({{ $value | humanize }} errors in the last 5 minutes).",
                                    "summary": "Kubelet has failed to renew its server certificate."
                                },
                                "expr": "increase(kubelet_server_expiration_renew_errors[5m]) \u003e 0\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "KubeletDown",
                                "annotations": {
                                    "description": "Kubelet has disappeared from Prometheus target discovery.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/KubeletDown.md",
                                    "summary": "Target disappeared from Prometheus target discovery."
                                },
                                "expr": "absent(up{job=\"kubelet\", metrics_path=\"/metrics\"} == 1)\n",
                                "for": "15m",
                                "labels": {
                                    "namespace": "kube-system",
                                    "severity": "critical"
                                }
                            }
                        ]
                    },
                    {
                        "name": "k8s.rules.container_cpu_usage_seconds_total",
                        "rules": [
                            {
                                "expr": "sum by (cluster, namespace, pod, container) (\n  irate(container_cpu_usage_seconds_total{job=\"kubelet\", metrics_path=\"/metrics/cadvisor\", image!=\"\"}[5m])\n) * on (cluster, namespace, pod) group_left(node) topk by (cluster, namespace, pod) (\n  1, max by(cluster, namespace, pod, node) (kube_pod_info{node!=\"\"})\n)\n",
                                "record": "node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate"
                            }
                        ]
                    },
                    {
                        "name": "k8s.rules.container_memory_working_set_bytes",
                        "rules": [
                            {
                                "expr": "container_memory_working_set_bytes{job=\"kubelet\", metrics_path=\"/metrics/cadvisor\", image!=\"\"}\n* on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,\n  max by(cluster, namespace, pod, node) (kube_pod_info{node!=\"\"})\n)\n",
                                "record": "node_namespace_pod_container:container_memory_working_set_bytes"
                            }
                        ]
                    },
                    {
                        "name": "k8s.rules.container_memory_rss",
                        "rules": [
                            {
                                "expr": "container_memory_rss{job=\"kubelet\", metrics_path=\"/metrics/cadvisor\", image!=\"\"}\n* on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,\n  max by(cluster, namespace, pod, node) (kube_pod_info{node!=\"\"})\n)\n",
                                "record": "node_namespace_pod_container:container_memory_rss"
                            }
                        ]
                    },
                    {
                        "name": "k8s.rules.container_memory_cache",
                        "rules": [
                            {
                                "expr": "container_memory_cache{job=\"kubelet\", metrics_path=\"/metrics/cadvisor\", image!=\"\"}\n* on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,\n  max by(cluster, namespace, pod, node) (kube_pod_info{node!=\"\"})\n)\n",
                                "record": "node_namespace_pod_container:container_memory_cache"
                            }
                        ]
                    },
                    {
                        "name": "k8s.rules.container_memory_swap",
                        "rules": [
                            {
                                "expr": "container_memory_swap{job=\"kubelet\", metrics_path=\"/metrics/cadvisor\", image!=\"\"}\n* on (cluster, namespace, pod) group_left(node) topk by(cluster, namespace, pod) (1,\n  max by(cluster, namespace, pod, node) (kube_pod_info{node!=\"\"})\n)\n",
                                "record": "node_namespace_pod_container:container_memory_swap"
                            }
                        ]
                    },
                    {
                        "name": "k8s.rules.container_resource",
                        "rules": [
                            {
                                "expr": "kube_pod_container_resource_requests{resource=\"memory\",job=\"kube-state-metrics\"}  * on (namespace, pod, cluster)\ngroup_left() max by (namespace, pod, cluster) (\n  (kube_pod_status_phase{phase=~\"Pending|Running\"} == 1)\n)\n",
                                "record": "cluster:namespace:pod_memory:active:kube_pod_container_resource_requests"
                            },
                            {
                                "expr": "sum by (namespace, cluster) (\n    sum by (namespace, pod, cluster) (\n        max by (namespace, pod, container, cluster) (\n          kube_pod_container_resource_requests{resource=\"memory\",job=\"kube-state-metrics\"}\n        ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (\n          kube_pod_status_phase{phase=~\"Pending|Running\"} == 1\n        )\n    )\n)\n",
                                "record": "namespace_memory:kube_pod_container_resource_requests:sum"
                            },
                            {
                                "expr": "kube_pod_container_resource_requests{resource=\"cpu\",job=\"kube-state-metrics\"}  * on (namespace, pod, cluster)\ngroup_left() max by (namespace, pod, cluster) (\n  (kube_pod_status_phase{phase=~\"Pending|Running\"} == 1)\n)\n",
                                "record": "cluster:namespace:pod_cpu:active:kube_pod_container_resource_requests"
                            },
                            {
                                "expr": "sum by (namespace, cluster) (\n    sum by (namespace, pod, cluster) (\n        max by (namespace, pod, container, cluster) (\n          kube_pod_container_resource_requests{resource=\"cpu\",job=\"kube-state-metrics\"}\n        ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (\n          kube_pod_status_phase{phase=~\"Pending|Running\"} == 1\n        )\n    )\n)\n",
                                "record": "namespace_cpu:kube_pod_container_resource_requests:sum"
                            },
                            {
                                "expr": "kube_pod_container_resource_limits{resource=\"memory\",job=\"kube-state-metrics\"}  * on (namespace, pod, cluster)\ngroup_left() max by (namespace, pod, cluster) (\n  (kube_pod_status_phase{phase=~\"Pending|Running\"} == 1)\n)\n",
                                "record": "cluster:namespace:pod_memory:active:kube_pod_container_resource_limits"
                            },
                            {
                                "expr": "sum by (namespace, cluster) (\n    sum by (namespace, pod, cluster) (\n        max by (namespace, pod, container, cluster) (\n          kube_pod_container_resource_limits{resource=\"memory\",job=\"kube-state-metrics\"}\n        ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (\n          kube_pod_status_phase{phase=~\"Pending|Running\"} == 1\n        )\n    )\n)\n",
                                "record": "namespace_memory:kube_pod_container_resource_limits:sum"
                            },
                            {
                                "expr": "kube_pod_container_resource_limits{resource=\"cpu\",job=\"kube-state-metrics\"}  * on (namespace, pod, cluster)\ngroup_left() max by (namespace, pod, cluster) (\n (kube_pod_status_phase{phase=~\"Pending|Running\"} == 1)\n )\n",
                                "record": "cluster:namespace:pod_cpu:active:kube_pod_container_resource_limits"
                            },
                            {
                                "expr": "sum by (namespace, cluster) (\n    sum by (namespace, pod, cluster) (\n        max by (namespace, pod, container, cluster) (\n          kube_pod_container_resource_limits{resource=\"cpu\",job=\"kube-state-metrics\"}\n        ) * on(namespace, pod, cluster) group_left() max by (namespace, pod, cluster) (\n          kube_pod_status_phase{phase=~\"Pending|Running\"} == 1\n        )\n    )\n)\n",
                                "record": "namespace_cpu:kube_pod_container_resource_limits:sum"
                            }
                        ]
                    },
                    {
                        "name": "k8s.rules.pod_owner",
                        "rules": [
                            {
                                "expr": "max by (cluster, namespace, workload, pod) (\n  label_replace(\n    label_replace(\n      kube_pod_owner{job=\"kube-state-metrics\", owner_kind=\"ReplicaSet\"},\n      \"replicaset\", \"$1\", \"owner_name\", \"(.*)\"\n    ) * on(replicaset, namespace) group_left(owner_name) topk by(replicaset, namespace) (\n      1, max by (replicaset, namespace, owner_name) (\n        kube_replicaset_owner{job=\"kube-state-metrics\"}\n      )\n    ),\n    \"workload\", \"$1\", \"owner_name\", \"(.*)\"\n  )\n)\n",
                                "labels": {
                                    "workload_type": "deployment"
                                },
                                "record": "namespace_workload_pod:kube_pod_owner:relabel"
                            },
                            {
                                "expr": "max by (cluster, namespace, workload, pod) (\n  label_replace(\n    kube_pod_owner{job=\"kube-state-metrics\", owner_kind=\"DaemonSet\"},\n    \"workload\", \"$1\", \"owner_name\", \"(.*)\"\n  )\n)\n",
                                "labels": {
                                    "workload_type": "daemonset"
                                },
                                "record": "namespace_workload_pod:kube_pod_owner:relabel"
                            },
                            {
                                "expr": "max by (cluster, namespace, workload, pod) (\n  label_replace(\n    kube_pod_owner{job=\"kube-state-metrics\", owner_kind=\"StatefulSet\"},\n    \"workload\", \"$1\", \"owner_name\", \"(.*)\"\n  )\n)\n",
                                "labels": {
                                    "workload_type": "statefulset"
                                },
                                "record": "namespace_workload_pod:kube_pod_owner:relabel"
                            },
                            {
                                "expr": "max by (cluster, namespace, workload, pod) (\n  label_replace(\n    kube_pod_owner{job=\"kube-state-metrics\", owner_kind=\"Job\"},\n    \"workload\", \"$1\", \"owner_name\", \"(.*)\"\n  )\n)\n",
                                "labels": {
                                    "workload_type": "job"
                                },
                                "record": "namespace_workload_pod:kube_pod_owner:relabel"
                            }
                        ]
                    },
                    {
                        "name": "kube-scheduler.rules",
                        "rules": [
                            {
                                "expr": "histogram_quantile(0.99, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job=\"scheduler\"}[5m])) without(instance, pod))\n",
                                "labels": {
                                    "quantile": "0.99"
                                },
                                "record": "cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile"
                            },
                            {
                                "expr": "histogram_quantile(0.99, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job=\"scheduler\"}[5m])) without(instance, pod))\n",
                                "labels": {
                                    "quantile": "0.99"
                                },
                                "record": "cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile"
                            },
                            {
                                "expr": "histogram_quantile(0.99, sum(rate(scheduler_binding_duration_seconds_bucket{job=\"scheduler\"}[5m])) without(instance, pod))\n",
                                "labels": {
                                    "quantile": "0.99"
                                },
                                "record": "cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile"
                            },
                            {
                                "expr": "histogram_quantile(0.9, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job=\"scheduler\"}[5m])) without(instance, pod))\n",
                                "labels": {
                                    "quantile": "0.9"
                                },
                                "record": "cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile"
                            },
                            {
                                "expr": "histogram_quantile(0.9, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job=\"scheduler\"}[5m])) without(instance, pod))\n",
                                "labels": {
                                    "quantile": "0.9"
                                },
                                "record": "cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile"
                            },
                            {
                                "expr": "histogram_quantile(0.9, sum(rate(scheduler_binding_duration_seconds_bucket{job=\"scheduler\"}[5m])) without(instance, pod))\n",
                                "labels": {
                                    "quantile": "0.9"
                                },
                                "record": "cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile"
                            },
                            {
                                "expr": "histogram_quantile(0.5, sum(rate(scheduler_e2e_scheduling_duration_seconds_bucket{job=\"scheduler\"}[5m])) without(instance, pod))\n",
                                "labels": {
                                    "quantile": "0.5"
                                },
                                "record": "cluster_quantile:scheduler_e2e_scheduling_duration_seconds:histogram_quantile"
                            },
                            {
                                "expr": "histogram_quantile(0.5, sum(rate(scheduler_scheduling_algorithm_duration_seconds_bucket{job=\"scheduler\"}[5m])) without(instance, pod))\n",
                                "labels": {
                                    "quantile": "0.5"
                                },
                                "record": "cluster_quantile:scheduler_scheduling_algorithm_duration_seconds:histogram_quantile"
                            },
                            {
                                "expr": "histogram_quantile(0.5, sum(rate(scheduler_binding_duration_seconds_bucket{job=\"scheduler\"}[5m])) without(instance, pod))\n",
                                "labels": {
                                    "quantile": "0.5"
                                },
                                "record": "cluster_quantile:scheduler_binding_duration_seconds:histogram_quantile"
                            }
                        ]
                    },
                    {
                        "name": "node.rules",
                        "rules": [
                            {
                                "expr": "topk by(cluster, namespace, pod) (1,\n  max by (cluster, node, namespace, pod) (\n    label_replace(kube_pod_info{job=\"kube-state-metrics\",node!=\"\"}, \"pod\", \"$1\", \"pod\", \"(.*)\")\n))\n",
                                "record": "node_namespace_pod:kube_pod_info:"
                            },
                            {
                                "expr": "sum(\n  node_memory_MemAvailable_bytes{job=\"node-exporter\"} or\n  (\n    node_memory_Buffers_bytes{job=\"node-exporter\"} +\n    node_memory_Cached_bytes{job=\"node-exporter\"} +\n    node_memory_MemFree_bytes{job=\"node-exporter\"} +\n    node_memory_Slab_bytes{job=\"node-exporter\"}\n  )\n) by (cluster)\n",
                                "record": ":node_memory_MemAvailable_bytes:sum"
                            },
                            {
                                "expr": "avg by (cluster, node) (\n  sum without (mode) (\n    rate(node_cpu_seconds_total{mode!=\"idle\",mode!=\"iowait\",mode!=\"steal\",job=\"node-exporter\"}[5m])\n  )\n)\n",
                                "record": "node:node_cpu_utilization:ratio_rate5m"
                            },
                            {
                                "expr": "avg by (cluster) (\n  node:node_cpu_utilization:ratio_rate5m\n)\n",
                                "record": "cluster:node_cpu:ratio_rate5m"
                            }
                        ]
                    },
                    {
                        "name": "kubelet.rules",
                        "rules": [
                            {
                                "expr": "histogram_quantile(0.99, sum(rate(kubelet_pleg_relist_duration_seconds_bucket{job=\"kubelet\", metrics_path=\"/metrics\"}[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job=\"kubelet\", metrics_path=\"/metrics\"})\n",
                                "labels": {
                                    "quantile": "0.99"
                                },
                                "record": "node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile"
                            },
                            {
                                "expr": "histogram_quantile(0.9, sum(rate(kubelet_pleg_relist_duration_seconds_bucket{job=\"kubelet\", metrics_path=\"/metrics\"}[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job=\"kubelet\", metrics_path=\"/metrics\"})\n",
                                "labels": {
                                    "quantile": "0.9"
                                },
                                "record": "node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile"
                            },
                            {
                                "expr": "histogram_quantile(0.5, sum(rate(kubelet_pleg_relist_duration_seconds_bucket{job=\"kubelet\", metrics_path=\"/metrics\"}[5m])) by (cluster, instance, le) * on(cluster, instance) group_left(node) kubelet_node_name{job=\"kubelet\", metrics_path=\"/metrics\"})\n",
                                "labels": {
                                    "quantile": "0.5"
                                },
                                "record": "node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile"
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "creationTimestamp": "2025-12-02T13:30:15Z",
                "generation": 1,
                "labels": {
                    "app.kubernetes.io/component": "exporter",
                    "app.kubernetes.io/managed-by": "cluster-monitoring-operator",
                    "app.kubernetes.io/name": "node-exporter",
                    "app.kubernetes.io/part-of": "openshift-monitoring",
                    "app.kubernetes.io/version": "1.8.2",
                    "prometheus": "k8s",
                    "role": "alert-rules"
                },
                "name": "node-exporter-rules",
                "namespace": "openshift-monitoring",
                "resourceVersion": "10172",
                "uid": "7b8871e3-9f01-4f37-b539-b3b42fdea9ea"
            },
            "spec": {
                "groups": [
                    {
                        "name": "node-exporter",
                        "rules": [
                            {
                                "alert": "NodeFilesystemSpaceFillingUp",
                                "annotations": {
                                    "description": "Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemSpaceFillingUp.md",
                                    "summary": "Filesystem is predicted to run out of space within the next 24 hours."
                                },
                                "expr": "(\n  node_filesystem_avail_bytes{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"} / node_filesystem_size_bytes{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"} * 100 \u003c 15\nand\n  predict_linear(node_filesystem_avail_bytes{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"}[6h], 24*60*60) \u003c 0\nand\n  node_filesystem_readonly{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"} == 0\n)\n",
                                "for": "1h",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "NodeFilesystemSpaceFillingUp",
                                "annotations": {
                                    "description": "Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left and is filling up fast.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemSpaceFillingUp.md",
                                    "summary": "Filesystem is predicted to run out of space within the next 4 hours."
                                },
                                "expr": "(\n  node_filesystem_avail_bytes{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"} / node_filesystem_size_bytes{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"} * 100 \u003c 10\nand\n  predict_linear(node_filesystem_avail_bytes{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"}[6h], 4*60*60) \u003c 0\nand\n  node_filesystem_readonly{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"} == 0\n)\n",
                                "for": "1h",
                                "labels": {
                                    "severity": "critical"
                                }
                            },
                            {
                                "alert": "NodeFilesystemAlmostOutOfSpace",
                                "annotations": {
                                    "description": "Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfSpace.md",
                                    "summary": "Filesystem has less than 5% space left."
                                },
                                "expr": "(\n  node_filesystem_avail_bytes{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"} / node_filesystem_size_bytes{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"} * 100 \u003c 5\nand\n  node_filesystem_readonly{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"} == 0\n)\n",
                                "for": "30m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "NodeFilesystemAlmostOutOfSpace",
                                "annotations": {
                                    "description": "Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available space left.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfSpace.md",
                                    "summary": "Filesystem has less than 3% space left."
                                },
                                "expr": "(\n  node_filesystem_avail_bytes{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"} / node_filesystem_size_bytes{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"} * 100 \u003c 3\nand\n  node_filesystem_readonly{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"} == 0\n)\n",
                                "for": "30m",
                                "labels": {
                                    "severity": "critical"
                                }
                            },
                            {
                                "alert": "NodeFilesystemFilesFillingUp",
                                "annotations": {
                                    "description": "Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemFilesFillingUp.md",
                                    "summary": "Filesystem is predicted to run out of inodes within the next 24 hours."
                                },
                                "expr": "(\n  node_filesystem_files_free{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"} / node_filesystem_files{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"} * 100 \u003c 40\nand\n  predict_linear(node_filesystem_files_free{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"}[6h], 24*60*60) \u003c 0\nand\n  node_filesystem_readonly{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"} == 0\n)\n",
                                "for": "1h",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "NodeFilesystemFilesFillingUp",
                                "annotations": {
                                    "description": "Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left and is filling up fast.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemFilesFillingUp.md",
                                    "summary": "Filesystem is predicted to run out of inodes within the next 4 hours."
                                },
                                "expr": "(\n  node_filesystem_files_free{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"} / node_filesystem_files{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"} * 100 \u003c 20\nand\n  predict_linear(node_filesystem_files_free{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"}[6h], 4*60*60) \u003c 0\nand\n  node_filesystem_readonly{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"} == 0\n)\n",
                                "for": "1h",
                                "labels": {
                                    "severity": "critical"
                                }
                            },
                            {
                                "alert": "NodeFilesystemAlmostOutOfFiles",
                                "annotations": {
                                    "description": "Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfFiles.md",
                                    "summary": "Filesystem has less than 5% inodes left."
                                },
                                "expr": "(\n  node_filesystem_files_free{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"} / node_filesystem_files{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"} * 100 \u003c 5\nand\n  node_filesystem_readonly{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"} == 0\n)\n",
                                "for": "1h",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "NodeFilesystemAlmostOutOfFiles",
                                "annotations": {
                                    "description": "Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf \"%.2f\" $value }}% available inodes left.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemAlmostOutOfFiles.md",
                                    "summary": "Filesystem has less than 3% inodes left."
                                },
                                "expr": "(\n  node_filesystem_files_free{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"} / node_filesystem_files{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"} * 100 \u003c 3\nand\n  node_filesystem_readonly{job=\"node-exporter\",fstype!=\"\",mountpoint!~\"/var/lib/ibmc-s3fs.*\"} == 0\n)\n",
                                "for": "1h",
                                "labels": {
                                    "severity": "critical"
                                }
                            },
                            {
                                "alert": "NodeNetworkReceiveErrs",
                                "annotations": {
                                    "description": "{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last two minutes.",
                                    "summary": "Network interface is reporting many receive errors."
                                },
                                "expr": "rate(node_network_receive_errs_total{job=\"node-exporter\"}[2m]) / rate(node_network_receive_packets_total{job=\"node-exporter\"}[2m]) \u003e 0.01\n",
                                "for": "1h",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "NodeNetworkTransmitErrs",
                                "annotations": {
                                    "description": "{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last two minutes.",
                                    "summary": "Network interface is reporting many transmit errors."
                                },
                                "expr": "rate(node_network_transmit_errs_total{job=\"node-exporter\"}[2m]) / rate(node_network_transmit_packets_total{job=\"node-exporter\"}[2m]) \u003e 0.01\n",
                                "for": "1h",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "NodeHighNumberConntrackEntriesUsed",
                                "annotations": {
                                    "description": "{{ $value | humanizePercentage }} of conntrack entries are used.",
                                    "summary": "Number of conntrack are getting close to the limit."
                                },
                                "expr": "(node_nf_conntrack_entries{job=\"node-exporter\"} / node_nf_conntrack_entries_limit) \u003e 0.75\n",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "NodeTextFileCollectorScrapeError",
                                "annotations": {
                                    "description": "Node Exporter text file collector on {{ $labels.instance }} failed to scrape.",
                                    "summary": "Node Exporter text file collector failed to scrape."
                                },
                                "expr": "node_textfile_scrape_error{job=\"node-exporter\"} == 1\n",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "NodeClockSkewDetected",
                                "annotations": {
                                    "description": "Clock at {{ $labels.instance }} is out of sync by more than 0.05s. Ensure NTP is configured correctly on this host.",
                                    "summary": "Clock skew detected."
                                },
                                "expr": "(\n(\n  node_timex_offset_seconds{job=\"node-exporter\"} \u003e 0.05\nand\n  deriv(node_timex_offset_seconds{job=\"node-exporter\"}[5m]) \u003e= 0\n)\nor\n(\n  node_timex_offset_seconds{job=\"node-exporter\"} \u003c -0.05\nand\n  deriv(node_timex_offset_seconds{job=\"node-exporter\"}[5m]) \u003c= 0\n)\n) and on() absent(up{job=\"ptp-monitor-service\"})",
                                "for": "10m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "NodeClockNotSynchronising",
                                "annotations": {
                                    "description": "Clock at {{ $labels.instance }} is not synchronising. Ensure NTP is configured on this host.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeClockNotSynchronising.md",
                                    "summary": "Clock not synchronising."
                                },
                                "expr": "(\nmin_over_time(node_timex_sync_status{job=\"node-exporter\"}[5m]) == 0\nand\nnode_timex_maxerror_seconds{job=\"node-exporter\"} \u003e= 16\n) and on() absent(up{job=\"ptp-monitor-service\"})",
                                "for": "10m",
                                "labels": {
                                    "severity": "critical"
                                }
                            },
                            {
                                "alert": "NodeRAIDDegraded",
                                "annotations": {
                                    "description": "RAID array '{{ $labels.device }}' at {{ $labels.instance }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeRAIDDegraded.md",
                                    "summary": "RAID Array is degraded."
                                },
                                "expr": "node_md_disks_required{job=\"node-exporter\",device=~\"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+\"} - ignoring (state) (node_md_disks{state=\"active\",job=\"node-exporter\",device=~\"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+\"}) \u003e 0\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "critical"
                                }
                            },
                            {
                                "alert": "NodeRAIDDiskFailure",
                                "annotations": {
                                    "description": "At least one device in RAID array at {{ $labels.instance }} failed. Array '{{ $labels.device }}' needs attention and possibly a disk swap.",
                                    "summary": "Failed device in RAID array."
                                },
                                "expr": "node_md_disks{state=\"failed\",job=\"node-exporter\",device=~\"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+\"} \u003e 0\n",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "NodeFileDescriptorLimit",
                                "annotations": {
                                    "description": "File descriptors limit at {{ $labels.instance }} is currently at {{ printf \"%.2f\" $value }}%.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFileDescriptorLimit.md",
                                    "summary": "Kernel is predicted to exhaust file descriptors limit soon."
                                },
                                "expr": "(\n  node_filefd_allocated{job=\"node-exporter\"} * 100 / node_filefd_maximum{job=\"node-exporter\"} \u003e 70\n)\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "NodeFileDescriptorLimit",
                                "annotations": {
                                    "description": "File descriptors limit at {{ $labels.instance }} is currently at {{ printf \"%.2f\" $value }}%.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFileDescriptorLimit.md",
                                    "summary": "Kernel is predicted to exhaust file descriptors limit soon."
                                },
                                "expr": "(\n  node_filefd_allocated{job=\"node-exporter\"} * 100 / node_filefd_maximum{job=\"node-exporter\"} \u003e 90\n)\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "critical"
                                }
                            },
                            {
                                "alert": "NodeSystemSaturation",
                                "annotations": {
                                    "description": "System load per core at {{ $labels.instance }} has been above 2 for the last 15 minutes, is currently at {{ printf \"%.2f\" $value }}.\nThis might indicate this instance resources saturation and can cause it becoming unresponsive.\n",
                                    "summary": "System saturated, load per core is very high."
                                },
                                "expr": "node_load1{job=\"node-exporter\"}\n/ count without (cpu, mode) (node_cpu_seconds_total{job=\"node-exporter\", mode=\"idle\"}) \u003e 2\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "NodeMemoryMajorPagesFaults",
                                "annotations": {
                                    "description": "Memory major pages are occurring at very high rate at {{ $labels.instance }}, 500 major page faults per second for the last 15 minutes, is currently at {{ printf \"%.2f\" $value }}.\nPlease check that there is enough memory available at this instance.\n",
                                    "summary": "Memory major page faults are occurring at very high rate."
                                },
                                "expr": "rate(node_vmstat_pgmajfault{job=\"node-exporter\"}[5m]) \u003e 500\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "NodeSystemdServiceFailed",
                                "annotations": {
                                    "description": "Systemd service {{ $labels.name }} has entered failed state at {{ $labels.instance }}",
                                    "summary": "Systemd service has entered failed state."
                                },
                                "expr": "node_systemd_unit_state{job=\"node-exporter\", state=\"failed\"} == 1\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "NodeBondingDegraded",
                                "annotations": {
                                    "description": "Bonding interface {{ $labels.master }} on {{ $labels.instance }} is in degraded state due to one or more slave failures.",
                                    "summary": "Bonding interface is degraded"
                                },
                                "expr": "(node_bonding_slaves - node_bonding_active) != 0\n",
                                "for": "5m",
                                "labels": {
                                    "severity": "warning"
                                }
                            }
                        ]
                    },
                    {
                        "name": "node-exporter.rules",
                        "rules": [
                            {
                                "expr": "count without (cpu, mode) (\n  node_cpu_seconds_total{job=\"node-exporter\",mode=\"idle\"}\n)\n",
                                "record": "instance:node_num_cpu:sum"
                            },
                            {
                                "expr": "1 - avg without (cpu) (\n  sum without (mode) (rate(node_cpu_seconds_total{job=\"node-exporter\", mode=~\"idle|iowait|steal\"}[1m]))\n)\n",
                                "record": "instance:node_cpu_utilisation:rate1m"
                            },
                            {
                                "expr": "(\n  node_load1{job=\"node-exporter\"}\n/\n  instance:node_num_cpu:sum{job=\"node-exporter\"}\n)\n",
                                "record": "instance:node_load1_per_cpu:ratio"
                            },
                            {
                                "expr": "1 - (\n  (\n    node_memory_MemAvailable_bytes{job=\"node-exporter\"}\n    or\n    (\n      node_memory_Buffers_bytes{job=\"node-exporter\"}\n      +\n      node_memory_Cached_bytes{job=\"node-exporter\"}\n      +\n      node_memory_MemFree_bytes{job=\"node-exporter\"}\n      +\n      node_memory_Slab_bytes{job=\"node-exporter\"}\n    )\n  )\n/\n  node_memory_MemTotal_bytes{job=\"node-exporter\"}\n)\n",
                                "record": "instance:node_memory_utilisation:ratio"
                            },
                            {
                                "expr": "rate(node_vmstat_pgmajfault{job=\"node-exporter\"}[1m])\n",
                                "record": "instance:node_vmstat_pgmajfault:rate1m"
                            },
                            {
                                "expr": "rate(node_disk_io_time_seconds_total{job=\"node-exporter\", device=~\"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+\"}[1m])\n",
                                "record": "instance_device:node_disk_io_time_seconds:rate1m"
                            },
                            {
                                "expr": "rate(node_disk_io_time_weighted_seconds_total{job=\"node-exporter\", device=~\"mmcblk.p.+|nvme.+|sd.+|vd.+|xvd.+|dm-.+|dasd.+\"}[1m])\n",
                                "record": "instance_device:node_disk_io_time_weighted_seconds:rate1m"
                            },
                            {
                                "expr": "sum without (device) (\n  rate(node_network_receive_bytes_total{job=\"node-exporter\", device!=\"lo\"}[1m])\n)\n",
                                "record": "instance:node_network_receive_bytes_excluding_lo:rate1m"
                            },
                            {
                                "expr": "sum without (device) (\n  rate(node_network_transmit_bytes_total{job=\"node-exporter\", device!=\"lo\"}[1m])\n)\n",
                                "record": "instance:node_network_transmit_bytes_excluding_lo:rate1m"
                            },
                            {
                                "expr": "sum without (device) (\n  rate(node_network_receive_drop_total{job=\"node-exporter\", device!=\"lo\"}[1m])\n)\n",
                                "record": "instance:node_network_receive_drop_excluding_lo:rate1m"
                            },
                            {
                                "expr": "sum without (device) (\n  rate(node_network_transmit_drop_total{job=\"node-exporter\", device!=\"lo\"}[1m])\n)\n",
                                "record": "instance:node_network_transmit_drop_excluding_lo:rate1m"
                            }
                        ]
                    },
                    {
                        "name": "telemetry",
                        "rules": [
                            {
                                "expr": "sum by(vendor,model) (node_accelerator_card_info)",
                                "record": "vendor_model:node_accelerator_cards:sum"
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "creationTimestamp": "2025-12-02T13:30:14Z",
                "generation": 1,
                "labels": {
                    "app.kubernetes.io/component": "prometheus",
                    "app.kubernetes.io/instance": "k8s",
                    "app.kubernetes.io/managed-by": "cluster-monitoring-operator",
                    "app.kubernetes.io/name": "prometheus",
                    "app.kubernetes.io/part-of": "openshift-monitoring",
                    "app.kubernetes.io/version": "2.55.1",
                    "prometheus": "k8s",
                    "role": "alert-rules"
                },
                "name": "prometheus-k8s-prometheus-rules",
                "namespace": "openshift-monitoring",
                "resourceVersion": "10117",
                "uid": "535be696-3463-4fbc-855f-9afd9a4c0e8b"
            },
            "spec": {
                "groups": [
                    {
                        "name": "prometheus",
                        "rules": [
                            {
                                "alert": "PrometheusBadConfig",
                                "annotations": {
                                    "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to reload its configuration.",
                                    "summary": "Failed Prometheus configuration reload."
                                },
                                "expr": "# Without max_over_time, failed scrapes could create false negatives, see\n# https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.\nmax_over_time(prometheus_config_last_reload_successful{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) == 0\n",
                                "for": "10m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "PrometheusSDRefreshFailure",
                                "annotations": {
                                    "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to refresh SD with mechanism {{$labels.mechanism}}.",
                                    "summary": "Failed Prometheus SD refresh."
                                },
                                "expr": "increase(prometheus_sd_refresh_failures_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[10m]) \u003e 0\n",
                                "for": "20m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "PrometheusKubernetesListWatchFailures",
                                "annotations": {
                                    "description": "Kubernetes service discovery of Prometheus {{$labels.namespace}}/{{$labels.pod}} is experiencing {{ printf \"%.0f\" $value }} failures with LIST/WATCH requests to the Kubernetes API in the last 5 minutes.",
                                    "summary": "Requests in Kubernetes SD are failing."
                                },
                                "expr": "increase(prometheus_sd_kubernetes_failures_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) \u003e 0\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "PrometheusNotificationQueueRunningFull",
                                "annotations": {
                                    "description": "Alert notification queue of Prometheus {{$labels.namespace}}/{{$labels.pod}} is running full.",
                                    "summary": "Prometheus alert notification queue predicted to run full in less than 30m."
                                },
                                "expr": "# Without min_over_time, failed scrapes could create false negatives, see\n# https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.\n(\n  predict_linear(prometheus_notifications_queue_length{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m], 60 * 30)\n\u003e\n  min_over_time(prometheus_notifications_queue_capacity{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m])\n)\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "PrometheusErrorSendingAlertsToSomeAlertmanagers",
                                "annotations": {
                                    "description": "{{ printf \"%.1f\" $value }}% errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.alertmanager}}.",
                                    "summary": "Prometheus has encountered more than 1% errors sending alerts to a specific Alertmanager."
                                },
                                "expr": "(\n  rate(prometheus_notifications_errors_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m])\n/\n  rate(prometheus_notifications_sent_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m])\n)\n* 100\n\u003e 1\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "PrometheusNotConnectedToAlertmanagers",
                                "annotations": {
                                    "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} is not connected to any Alertmanagers.",
                                    "summary": "Prometheus is not connected to any Alertmanagers."
                                },
                                "expr": "# Without max_over_time, failed scrapes could create false negatives, see\n# https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.\nmax_over_time(prometheus_notifications_alertmanagers_discovered{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) \u003c 1\n",
                                "for": "10m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "PrometheusTSDBReloadsFailing",
                                "annotations": {
                                    "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} reload failures over the last 3h.",
                                    "summary": "Prometheus has issues reloading blocks from disk."
                                },
                                "expr": "increase(prometheus_tsdb_reloads_failures_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[3h]) \u003e 0\n",
                                "for": "4h",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "PrometheusTSDBCompactionsFailing",
                                "annotations": {
                                    "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} compaction failures over the last 3h.",
                                    "summary": "Prometheus has issues compacting blocks."
                                },
                                "expr": "increase(prometheus_tsdb_compactions_failed_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[3h]) \u003e 0\n",
                                "for": "4h",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "PrometheusNotIngestingSamples",
                                "annotations": {
                                    "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} is not ingesting samples.",
                                    "summary": "Prometheus is not ingesting samples."
                                },
                                "expr": "(\n  sum without(type) (rate(prometheus_tsdb_head_samples_appended_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m])) \u003c= 0\nand\n  (\n    sum without(scrape_job) (prometheus_target_metadata_cache_entries{job=~\"prometheus-k8s|prometheus-user-workload\"}) \u003e 0\n  or\n    sum without(rule_group) (prometheus_rule_group_rules{job=~\"prometheus-k8s|prometheus-user-workload\"}) \u003e 0\n  )\n)\n",
                                "for": "10m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "PrometheusDuplicateTimestamps",
                                "annotations": {
                                    "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf \"%.4g\" $value  }} samples/s with different values but duplicated timestamp.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusDuplicateTimestamps.md",
                                    "summary": "Prometheus is dropping samples with duplicate timestamps."
                                },
                                "expr": "rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) \u003e 0\n",
                                "for": "1h",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "PrometheusOutOfOrderTimestamps",
                                "annotations": {
                                    "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf \"%.4g\" $value  }} samples/s with timestamps arriving out of order.",
                                    "summary": "Prometheus drops samples with out-of-order timestamps."
                                },
                                "expr": "rate(prometheus_target_scrapes_sample_out_of_order_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) \u003e 0\n",
                                "for": "1h",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "PrometheusRemoteStorageFailures",
                                "annotations": {
                                    "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} failed to send {{ printf \"%.1f\" $value }}% of the samples to {{ $labels.remote_name}}:{{ $labels.url }}",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusRemoteStorageFailures.md",
                                    "summary": "Prometheus fails to send samples to remote storage."
                                },
                                "expr": "(\n  (rate(prometheus_remote_storage_failed_samples_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]))\n/\n  (\n    (rate(prometheus_remote_storage_failed_samples_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]))\n  +\n    (rate(prometheus_remote_storage_succeeded_samples_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) or rate(prometheus_remote_storage_samples_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]))\n  )\n)\n* 100\n\u003e 1\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "PrometheusRemoteWriteBehind",
                                "annotations": {
                                    "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write is {{ printf \"%.1f\" $value }}s behind for {{ $labels.remote_name}}:{{ $labels.url }}.",
                                    "summary": "Prometheus remote write is behind."
                                },
                                "expr": "# Use the metric added in https://github.com/openshift/prometheus/pull/262 and related PRs.\n# Without max_over_time, failed scrapes could create false negatives, see\n# https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.\n(\n  max_over_time(prometheus_remote_storage_queue_highest_timestamp_seconds{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m])\n-\n  max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m])\n)\n\u003e 120\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "info"
                                }
                            },
                            {
                                "alert": "PrometheusRemoteWriteDesiredShards",
                                "annotations": {
                                    "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write desired shards calculation wants to run {{ $value }} shards for queue {{ $labels.remote_name}}:{{ $labels.url }}, which is more than the max of {{ printf `prometheus_remote_storage_shards_max{instance=\"%s\",job=~\"prometheus-k8s|prometheus-user-workload\"}` $labels.instance | query | first | value }}.",
                                    "summary": "Prometheus remote write desired shards calculation wants to run more than configured max shards."
                                },
                                "expr": "# Without max_over_time, failed scrapes could create false negatives, see\n# https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.\n(\n  max_over_time(prometheus_remote_storage_shards_desired{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m])\n\u003e\n  max_over_time(prometheus_remote_storage_shards_max{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m])\n)\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "PrometheusRuleFailures",
                                "annotations": {
                                    "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to evaluate {{ printf \"%.0f\" $value }} rules in the last 5m.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusRuleFailures.md",
                                    "summary": "Prometheus is failing rule evaluations."
                                },
                                "expr": "increase(prometheus_rule_evaluation_failures_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) \u003e 0\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "PrometheusMissingRuleEvaluations",
                                "annotations": {
                                    "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has missed {{ printf \"%.0f\" $value }} rule group evaluations in the last 5m.",
                                    "summary": "Prometheus is missing rule evaluations due to slow rule group evaluation."
                                },
                                "expr": "increase(prometheus_rule_group_iterations_missed_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) \u003e 0\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "PrometheusTargetLimitHit",
                                "annotations": {
                                    "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{ printf \"%.0f\" $value }} targets because the number of targets exceeded the configured target_limit.",
                                    "summary": "Prometheus has dropped targets because some scrape configs have exceeded the targets limit."
                                },
                                "expr": "increase(prometheus_target_scrape_pool_exceeded_target_limit_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) \u003e 0\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "PrometheusLabelLimitHit",
                                "annotations": {
                                    "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{ printf \"%.0f\" $value }} targets because some samples exceeded the configured label_limit, label_name_length_limit or label_value_length_limit.",
                                    "summary": "Prometheus has dropped targets because some scrape configs have exceeded the labels limit."
                                },
                                "expr": "increase(prometheus_target_scrape_pool_exceeded_label_limits_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) \u003e 0\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "PrometheusScrapeBodySizeLimitHit",
                                "annotations": {
                                    "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{ printf \"%.0f\" $value }} scrapes in the last 5m because some targets exceeded the configured body_size_limit.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusScrapeBodySizeLimitHit.md",
                                    "summary": "Prometheus has dropped some targets that exceeded body size limit."
                                },
                                "expr": "increase(prometheus_target_scrapes_exceeded_body_size_limit_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) \u003e 0\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "PrometheusScrapeSampleLimitHit",
                                "annotations": {
                                    "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{ printf \"%.0f\" $value }} scrapes in the last 5m because some targets exceeded the configured sample_limit.",
                                    "summary": "Prometheus has failed scrapes that have exceeded the configured sample limit."
                                },
                                "expr": "increase(prometheus_target_scrapes_exceeded_sample_limit_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) \u003e 0\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "PrometheusTargetSyncFailure",
                                "annotations": {
                                    "description": "{{ printf \"%.0f\" $value }} targets in Prometheus {{$labels.namespace}}/{{$labels.pod}} have failed to sync because invalid configuration was supplied.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusTargetSyncFailure.md",
                                    "summary": "Prometheus has failed to sync targets."
                                },
                                "expr": "increase(prometheus_target_sync_failed_total{job=~\"prometheus-k8s|prometheus-user-workload\"}[30m]) \u003e 0\n",
                                "for": "5m",
                                "labels": {
                                    "severity": "critical"
                                }
                            },
                            {
                                "alert": "PrometheusHighQueryLoad",
                                "annotations": {
                                    "description": "Prometheus {{$labels.namespace}}/{{$labels.pod}} query API has less than 20% available capacity in its query engine for the last 15 minutes.",
                                    "summary": "Prometheus is reaching its maximum capacity serving concurrent requests."
                                },
                                "expr": "avg_over_time(prometheus_engine_queries{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) / max_over_time(prometheus_engine_queries_concurrent_max{job=~\"prometheus-k8s|prometheus-user-workload\"}[5m]) \u003e 0.8\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "creationTimestamp": "2025-12-02T13:30:14Z",
                "generation": 1,
                "labels": {
                    "app.kubernetes.io/component": "prometheus",
                    "app.kubernetes.io/instance": "k8s",
                    "app.kubernetes.io/managed-by": "cluster-monitoring-operator",
                    "app.kubernetes.io/name": "prometheus",
                    "app.kubernetes.io/part-of": "openshift-monitoring",
                    "app.kubernetes.io/version": "2.55.1",
                    "prometheus": "k8s",
                    "role": "alert-rules"
                },
                "name": "prometheus-k8s-thanos-sidecar-rules",
                "namespace": "openshift-monitoring",
                "resourceVersion": "10120",
                "uid": "e51f2e08-bc1c-48d0-816d-1eb3617ab29d"
            },
            "spec": {
                "groups": [
                    {
                        "name": "thanos-sidecar",
                        "rules": [
                            {
                                "alert": "ThanosSidecarBucketOperationsFailed",
                                "annotations": {
                                    "description": "Thanos Sidecar {{$labels.instance}} in {{$labels.namespace}} bucket operations are failing",
                                    "summary": "Thanos Sidecar bucket operations are failing"
                                },
                                "expr": "sum by (namespace, job, instance) (rate(thanos_objstore_bucket_operation_failures_total{job=~\"prometheus-(k8s|user-workload)-thanos-sidecar\"}[5m])) \u003e 0\n",
                                "for": "1h",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "ThanosSidecarNoConnectionToStartedPrometheus",
                                "annotations": {
                                    "description": "Thanos Sidecar {{$labels.instance}} in {{$labels.namespace}} is unhealthy.",
                                    "summary": "Thanos Sidecar cannot access Prometheus, even though Prometheus seems healthy and has reloaded WAL."
                                },
                                "expr": "thanos_sidecar_prometheus_up{job=~\"prometheus-(k8s|user-workload)-thanos-sidecar\"} == 0\nAND on (namespace, pod)\nprometheus_tsdb_data_replay_duration_seconds != 0\n",
                                "for": "1h",
                                "labels": {
                                    "severity": "warning"
                                }
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "creationTimestamp": "2025-12-02T13:30:10Z",
                "generation": 1,
                "labels": {
                    "app.kubernetes.io/component": "controller",
                    "app.kubernetes.io/managed-by": "cluster-monitoring-operator",
                    "app.kubernetes.io/name": "prometheus-operator",
                    "app.kubernetes.io/part-of": "openshift-monitoring",
                    "app.kubernetes.io/version": "0.78.1",
                    "prometheus": "k8s",
                    "role": "alert-rules"
                },
                "name": "prometheus-operator-rules",
                "namespace": "openshift-monitoring",
                "resourceVersion": "9673",
                "uid": "9942fe7d-932b-4567-a950-12c3901ffec5"
            },
            "spec": {
                "groups": [
                    {
                        "name": "prometheus-operator",
                        "rules": [
                            {
                                "alert": "PrometheusOperatorListErrors",
                                "annotations": {
                                    "description": "Errors while performing List operations in controller {{$labels.controller}} in {{$labels.namespace}} namespace.",
                                    "summary": "Errors while performing list operations in controller."
                                },
                                "expr": "(sum by (cluster,controller,namespace) (rate(prometheus_operator_list_operations_failed_total{job=\"prometheus-operator\", namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[10m])) / sum by (cluster,controller,namespace) (rate(prometheus_operator_list_operations_total{job=\"prometheus-operator\", namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[10m]))) \u003e 0.4\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "PrometheusOperatorWatchErrors",
                                "annotations": {
                                    "description": "Errors while performing watch operations in controller {{$labels.controller}} in {{$labels.namespace}} namespace.",
                                    "summary": "Errors while performing watch operations in controller."
                                },
                                "expr": "(sum by (cluster,controller,namespace) (rate(prometheus_operator_watch_operations_failed_total{job=\"prometheus-operator\", namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[5m])) / sum by (cluster,controller,namespace) (rate(prometheus_operator_watch_operations_total{job=\"prometheus-operator\", namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[5m]))) \u003e 0.4\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "PrometheusOperatorSyncFailed",
                                "annotations": {
                                    "description": "Controller {{ $labels.controller }} in {{ $labels.namespace }} namespace fails to reconcile {{ $value }} objects.",
                                    "summary": "Last controller reconciliation failed"
                                },
                                "expr": "min_over_time(prometheus_operator_syncs{status=\"failed\",job=\"prometheus-operator\", namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[5m]) \u003e 0\n",
                                "for": "10m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "PrometheusOperatorReconcileErrors",
                                "annotations": {
                                    "description": "{{ $value | humanizePercentage }} of reconciling operations failed for {{ $labels.controller }} controller in {{ $labels.namespace }} namespace.",
                                    "summary": "Errors while reconciling objects."
                                },
                                "expr": "(sum by (cluster,controller,namespace) (rate(prometheus_operator_reconcile_errors_total{job=\"prometheus-operator\", namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[5m]))) / (sum by (cluster,controller,namespace) (rate(prometheus_operator_reconcile_operations_total{job=\"prometheus-operator\", namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[5m]))) \u003e 0.1\n",
                                "for": "10m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "PrometheusOperatorStatusUpdateErrors",
                                "annotations": {
                                    "description": "{{ $value | humanizePercentage }} of status update operations failed for {{ $labels.controller }} controller in {{ $labels.namespace }} namespace.",
                                    "summary": "Errors while updating objects status."
                                },
                                "expr": "(sum by (cluster,controller,namespace) (rate(prometheus_operator_status_update_errors_total{job=\"prometheus-operator\", namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[5m]))) / (sum by (cluster,controller,namespace) (rate(prometheus_operator_status_update_operations_total{job=\"prometheus-operator\", namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[5m]))) \u003e 0.1\n",
                                "for": "10m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "PrometheusOperatorNodeLookupErrors",
                                "annotations": {
                                    "description": "Errors while reconciling Prometheus in {{ $labels.namespace }} Namespace.",
                                    "summary": "Errors while reconciling Prometheus."
                                },
                                "expr": "rate(prometheus_operator_node_address_lookup_errors_total{job=\"prometheus-operator\", namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[5m]) \u003e 0.1\n",
                                "for": "10m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "PrometheusOperatorNotReady",
                                "annotations": {
                                    "description": "Prometheus operator in {{ $labels.namespace }} namespace isn't ready to reconcile {{ $labels.controller }} resources.",
                                    "summary": "Prometheus operator not ready"
                                },
                                "expr": "min by (cluster,controller,namespace) (max_over_time(prometheus_operator_ready{job=\"prometheus-operator\", namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[5m]) == 0)\n",
                                "for": "5m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "PrometheusOperatorRejectedResources",
                                "annotations": {
                                    "description": "Prometheus operator in {{ $labels.namespace }} namespace rejected {{ printf \"%0.0f\" $value }} {{ $labels.controller }}/{{ $labels.resource }} resources.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/PrometheusOperatorRejectedResources.md",
                                    "summary": "Resources rejected by Prometheus operator"
                                },
                                "expr": "min_over_time(prometheus_operator_managed_resources{state=\"rejected\",job=\"prometheus-operator\", namespace=~\"openshift-monitoring|openshift-user-workload-monitoring\"}[5m]) \u003e 0\n",
                                "for": "5m",
                                "labels": {
                                    "severity": "warning"
                                }
                            }
                        ]
                    },
                    {
                        "name": "config-reloaders",
                        "rules": [
                            {
                                "alert": "ConfigReloaderSidecarErrors",
                                "annotations": {
                                    "description": "Errors encountered while the {{$labels.pod}} config-reloader sidecar attempts to sync config in {{$labels.namespace}} namespace.\nAs a result, configuration for service running in {{$labels.pod}} may be stale and cannot be updated anymore.",
                                    "summary": "config-reloader sidecar has not had a successful reload for 10m"
                                },
                                "expr": "max_over_time(reloader_last_reload_successful{namespace=~\".+\"}[5m]) == 0\n",
                                "for": "10m",
                                "labels": {
                                    "severity": "warning"
                                }
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "creationTimestamp": "2025-12-02T13:31:49Z",
                "generation": 1,
                "labels": {
                    "app.kubernetes.io/managed-by": "cluster-monitoring-operator",
                    "app.kubernetes.io/part-of": "openshift-monitoring"
                },
                "name": "telemetry",
                "namespace": "openshift-monitoring",
                "resourceVersion": "12371",
                "uid": "3fbeeeec-d39b-41b9-88b5-38accd739037"
            },
            "spec": {
                "groups": [
                    {
                        "name": "telemeter.rules",
                        "rules": [
                            {
                                "expr": "max(federate_samples - federate_filtered_samples)",
                                "record": "cluster:telemetry_selected_series:count"
                            },
                            {
                                "alert": "TelemeterClientFailures",
                                "annotations": {
                                    "description": "The telemeter client in namespace {{ $labels.namespace }} fails {{ $value | humanize }} of the requests to the telemeter service.\nCheck the logs of the telemeter-client pod with the following command:\noc logs -n openshift-monitoring deployment.apps/telemeter-client -c telemeter-client\nIf the telemeter client fails to authenticate with the telemeter service, make sure that the global pull secret is up to date, see https://docs.openshift.com/container-platform/latest/openshift_images/managing_images/using-image-pull-secrets.html#images-update-global-pull-secret_using-image-pull-secrets for more details.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/TelemeterClientFailures.md",
                                    "summary": "Telemeter client fails to send metrics"
                                },
                                "expr": "sum by (namespace) (\n  rate(federate_requests_failed_total{job=\"telemeter-client\"}[15m])\n) /\nsum by (namespace) (\n  rate(federate_requests_total{job=\"telemeter-client\"}[15m])\n) \u003e 0.2\n",
                                "for": "1h",
                                "labels": {
                                    "severity": "warning"
                                }
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "creationTimestamp": "2025-12-02T13:30:42Z",
                "generation": 1,
                "labels": {
                    "app.kubernetes.io/component": "query-layer",
                    "app.kubernetes.io/instance": "thanos-querier",
                    "app.kubernetes.io/managed-by": "cluster-monitoring-operator",
                    "app.kubernetes.io/name": "thanos-query",
                    "app.kubernetes.io/part-of": "openshift-monitoring",
                    "app.kubernetes.io/version": "0.36.1"
                },
                "name": "thanos-querier",
                "namespace": "openshift-monitoring",
                "resourceVersion": "11467",
                "uid": "d3352781-be68-4d7e-b573-9e1e537ec669"
            },
            "spec": {
                "groups": [
                    {
                        "name": "thanos-query",
                        "rules": [
                            {
                                "alert": "ThanosQueryHttpRequestQueryErrorRateHigh",
                                "annotations": {
                                    "description": "Thanos Query {{$labels.job}} in {{$labels.namespace}} is failing to handle {{$value | humanize}}% of \"query\" requests.",
                                    "summary": "Thanos Query is failing to handle requests."
                                },
                                "expr": "(\n  sum by (namespace, job) (rate(http_requests_total{code=~\"5..\", job=\"thanos-querier\", handler=\"query\"}[5m]))\n/\n  sum by (namespace, job) (rate(http_requests_total{job=\"thanos-querier\", handler=\"query\"}[5m]))\n) * 100 \u003e 5\n",
                                "for": "1h",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "ThanosQueryHttpRequestQueryRangeErrorRateHigh",
                                "annotations": {
                                    "description": "Thanos Query {{$labels.job}} in {{$labels.namespace}} is failing to handle {{$value | humanize}}% of \"query_range\" requests.",
                                    "summary": "Thanos Query is failing to handle requests."
                                },
                                "expr": "(\n  sum by (namespace, job) (rate(http_requests_total{code=~\"5..\", job=\"thanos-querier\", handler=\"query_range\"}[5m]))\n/\n  sum by (namespace, job) (rate(http_requests_total{job=\"thanos-querier\", handler=\"query_range\"}[5m]))\n) * 100 \u003e 5\n",
                                "for": "1h",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "ThanosQueryGrpcServerErrorRate",
                                "annotations": {
                                    "description": "Thanos Query {{$labels.job}} in {{$labels.namespace}} is failing to handle {{$value | humanize}}% of requests.",
                                    "summary": "Thanos Query is failing to handle requests."
                                },
                                "expr": "(\n  sum by (namespace, job) (rate(grpc_server_handled_total{grpc_code=~\"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded\", job=\"thanos-querier\"}[5m]))\n/\n  sum by (namespace, job) (rate(grpc_server_started_total{job=\"thanos-querier\"}[5m]))\n* 100 \u003e 5\n)\n",
                                "for": "1h",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "ThanosQueryGrpcClientErrorRate",
                                "annotations": {
                                    "description": "Thanos Query {{$labels.job}} in {{$labels.namespace}} is failing to send {{$value | humanize}}% of requests.",
                                    "summary": "Thanos Query is failing to send requests."
                                },
                                "expr": "(\n  sum by (namespace, job) (rate(grpc_client_handled_total{grpc_code!=\"OK\", job=\"thanos-querier\"}[5m]))\n/\n  sum by (namespace, job) (rate(grpc_client_started_total{job=\"thanos-querier\"}[5m]))\n) * 100 \u003e 5\n",
                                "for": "1h",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "ThanosQueryHighDNSFailures",
                                "annotations": {
                                    "description": "Thanos Query {{$labels.job}} in {{$labels.namespace}} have {{$value | humanize}}% of failing DNS queries for store endpoints.",
                                    "summary": "Thanos Query is having high number of DNS failures."
                                },
                                "expr": "(\n  sum by (namespace, job) (rate(thanos_query_store_apis_dns_failures_total{job=\"thanos-querier\"}[5m]))\n/\n  sum by (namespace, job) (rate(thanos_query_store_apis_dns_lookups_total{job=\"thanos-querier\"}[5m]))\n) * 100 \u003e 1\n",
                                "for": "1h",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "ThanosQueryOverload",
                                "annotations": {
                                    "description": "Thanos Query {{$labels.job}} in {{$labels.namespace}} has been overloaded for more than 15 minutes. This may be a symptom of excessive simultaneous complex requests, low performance of the Prometheus API, or failures within these components. Assess the health of the Thanos query instances, the connected Prometheus instances, look for potential senders of these requests and then contact support.",
                                    "summary": "Thanos query reaches its maximum capacity serving concurrent requests."
                                },
                                "expr": "(\n  max_over_time(thanos_query_concurrent_gate_queries_max[5m]) - avg_over_time(thanos_query_concurrent_gate_queries_in_flight[5m]) \u003c 1\n)\n",
                                "for": "1h",
                                "labels": {
                                    "severity": "warning"
                                }
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "creationTimestamp": "2025-12-02T13:23:56Z",
                "generation": 1,
                "name": "openshift-network-operator-ipsec-rules",
                "namespace": "openshift-network-operator",
                "ownerReferences": [
                    {
                        "apiVersion": "operator.openshift.io/v1",
                        "blockOwnerDeletion": true,
                        "controller": true,
                        "kind": "Network",
                        "name": "cluster",
                        "uid": "838fef98-8efc-4e1a-b00a-d6f0864ceada"
                    }
                ],
                "resourceVersion": "3982",
                "uid": "ebe1a1ea-e751-4be6-8587-4ead49b7b4c5"
            },
            "spec": {
                "groups": [
                    {
                        "name": "openshift-network.rules",
                        "rules": [
                            {
                                "expr": "group by (mode,is_legacy_api) (\n  openshift_network_operator_ipsec_state{namespace=~\"openshift-network-operator\"}\n)",
                                "record": "openshift:openshift_network_operator_ipsec_state:info"
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "annotations": {
                    "capability.openshift.io/name": "OperatorLifecycleManager",
                    "include.release.openshift.io/hypershift": "true",
                    "include.release.openshift.io/ibm-cloud-managed": "true",
                    "include.release.openshift.io/self-managed-high-availability": "true"
                },
                "creationTimestamp": "2025-12-02T13:23:17Z",
                "generation": 1,
                "labels": {
                    "prometheus": "alert-rules",
                    "role": "alert-rules"
                },
                "name": "olm-alert-rules",
                "namespace": "openshift-operator-lifecycle-manager",
                "ownerReferences": [
                    {
                        "apiVersion": "config.openshift.io/v1",
                        "controller": true,
                        "kind": "ClusterVersion",
                        "name": "version",
                        "uid": "46b5b63c-dc72-48b6-9f35-f2dea4314c26"
                    }
                ],
                "resourceVersion": "1204",
                "uid": "c2361909-8963-446d-bf59-8798aa3c03d7"
            },
            "spec": {
                "groups": [
                    {
                        "name": "olm.csv_abnormal.rules",
                        "rules": [
                            {
                                "alert": "CsvAbnormalFailedOver2Min",
                                "annotations": {
                                    "description": "Failed to install Operator {{ $labels.name }} version {{ $labels.version }}. Reason-{{ $labels.reason }}",
                                    "summary": "CSV failed for over 2 minutes"
                                },
                                "expr": "last_over_time(csv_abnormal{phase=\"Failed\"}[5m])",
                                "for": "2m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "CsvAbnormalOver30Min",
                                "annotations": {
                                    "description": "Failed to install Operator {{ $labels.name }} version {{ $labels.version }}. Phase-{{ $labels.phase }} Reason-{{ $labels.reason }}",
                                    "summary": "CSV abnormal for over 30 minutes"
                                },
                                "expr": "last_over_time(csv_abnormal{phase=~\"(Replacing|Pending|Deleting|Unknown)\"}[5m])",
                                "for": "30m",
                                "labels": {
                                    "severity": "warning"
                                }
                            }
                        ]
                    },
                    {
                        "name": "olm.installplan.rules",
                        "rules": [
                            {
                                "alert": "InstallPlanStepAppliedWithWarnings",
                                "annotations": {
                                    "description": "The API server returned a warning during installation or upgrade of an operator. An Event with reason \"AppliedWithWarnings\" has been created with complete details, including a reference to the InstallPlan step that generated the warning.",
                                    "summary": "API returned a warning when modifying an operator"
                                },
                                "expr": "sum by(namespace) (increase(installplan_warnings_total[5m])) \u003e 0",
                                "labels": {
                                    "severity": "warning"
                                }
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "annotations": {
                    "networkoperator.openshift.io/ignore-errors": ""
                },
                "creationTimestamp": "2025-12-02T13:23:49Z",
                "generation": 1,
                "labels": {
                    "prometheus": "k8s",
                    "role": "alert-rules"
                },
                "name": "networking-rules",
                "namespace": "openshift-ovn-kubernetes",
                "ownerReferences": [
                    {
                        "apiVersion": "operator.openshift.io/v1",
                        "blockOwnerDeletion": true,
                        "controller": true,
                        "kind": "Network",
                        "name": "cluster",
                        "uid": "838fef98-8efc-4e1a-b00a-d6f0864ceada"
                    }
                ],
                "resourceVersion": "3777",
                "uid": "6b71608a-ba7f-4089-b04e-2bd230ced6f7"
            },
            "spec": {
                "groups": [
                    {
                        "name": "cluster-network-operator-ovn.rules",
                        "rules": [
                            {
                                "alert": "NodeWithoutOVNKubeNodePodRunning",
                                "annotations": {
                                    "description": "Networking is degraded on nodes that do not have a functioning ovnkube-node pod. Existing workloads on the\nnode may continue to have connectivity but any changes to the networking control plane will not be implemented.\n",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/NodeWithoutOVNKubeNodePodRunning.md",
                                    "summary": "All Linux nodes should be running an ovnkube-node pod, {{ $labels.node }} is not."
                                },
                                "expr": "(kube_node_info unless on(node) (kube_pod_info{namespace=\"openshift-ovn-kubernetes\",pod=~\"ovnkube-node.*\"}\nor kube_node_labels{label_kubernetes_io_os=\"windows\"})) \u003e 0\n",
                                "for": "20m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "OVNKubernetesControllerDisconnectedSouthboundDatabase",
                                "annotations": {
                                    "description": "Networking is degraded on nodes when OVN controller is not connected to OVN southbound database connection. No networking control plane updates will be applied to the node.\n",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/OVNKubernetesControllerDisconnectedSouthboundDatabase.md",
                                    "summary": "Networking control plane is degraded on node {{ $labels.node }} because OVN controller is not connected to OVN southbound database."
                                },
                                "expr": "max_over_time(ovn_controller_southbound_database_connected[5m]) == 0\n",
                                "for": "10m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "OVNKubernetesNodePodAddError",
                                "annotations": {
                                    "description": "OVN Kubernetes experiences pod creation errors at an elevated rate. The pods will be retried.",
                                    "summary": "OVN Kubernetes is experiencing pod creation errors at an elevated rate."
                                },
                                "expr": "(sum by(instance, namespace) (rate(ovnkube_node_cni_request_duration_seconds_count{command=\"ADD\",err=\"true\"}[5m]))\n  /\nsum by(instance, namespace) (rate(ovnkube_node_cni_request_duration_seconds_count{command=\"ADD\"}[5m])))\n\u003e 0.1\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "OVNKubernetesNodePodDeleteError",
                                "annotations": {
                                    "description": "OVN Kubernetes experiences pod deletion errors at an elevated rate. The pods will be retried.",
                                    "summary": "OVN Kubernetes experiencing pod deletion errors at an elevated rate."
                                },
                                "expr": "(sum by(instance, namespace) (rate(ovnkube_node_cni_request_duration_seconds_count{command=\"DEL\",err=\"true\"}[5m]))\n  /\nsum by(instance, namespace) (rate(ovnkube_node_cni_request_duration_seconds_count{command=\"DEL\"}[5m])))\n\u003e 0.1\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "OVNKubernetesResourceRetryFailure",
                                "annotations": {
                                    "description": "OVN Kubernetes failed to apply networking control plane configuration after several attempts. This might be because the configuration\nprovided by the user is invalid or because of an internal error. As a consequence, the cluster might have a degraded status.\n",
                                    "summary": "OVN Kubernetes failed to apply networking control plane configuration."
                                },
                                "expr": "increase(ovnkube_resource_retry_failures_total[10m]) \u003e 0",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "OVNKubernetesNodeOVSOverflowUserspace",
                                "annotations": {
                                    "description": "Netlink messages dropped by OVS vSwitch daemon due to netlink socket buffer overflow. This will result in packet loss.",
                                    "summary": "OVS vSwitch daemon drops packets due to buffer overflow."
                                },
                                "expr": "increase(ovs_vswitchd_netlink_overflow[5m]) \u003e 0",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "OVNKubernetesNodeOVSOverflowKernel",
                                "annotations": {
                                    "description": "Netlink messages dropped by OVS kernel module due to netlink socket buffer overflow. This will result in packet loss.",
                                    "summary": "OVS kernel module drops packets due to buffer overflow."
                                },
                                "expr": "increase(ovs_vswitchd_dp_flows_lookup_lost[5m]) \u003e 0",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "NorthboundStale",
                                "annotations": {
                                    "description": "OVN-Kubernetes controller and/or OVN northbound database may cause a\ndegraded networking control plane for the affected node. Existing\nworkloads should continue to have connectivity but new workloads may\nbe impacted.\n",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/NorthboundStaleAlert.md",
                                    "summary": "OVN-Kubernetes controller {{ $labels.instance }} has not successfully synced any changes to the northbound database for too long."
                                },
                                "expr": "# Without max_over_time, failed scrapes could create false negatives, see\n# https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.\ntime() - max_over_time(ovnkube_controller_nb_e2e_timestamp[5m]) \u003e 120\n",
                                "for": "10m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "SouthboundStale",
                                "annotations": {
                                    "description": "OVN-Kubernetes controller and/or OVN northbound database may cause a\ndegraded networking control plane for the affected node. Existing\nworkloads should continue to have connectivity but new workloads may\nbe impacted.\n",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/SouthboundStaleAlert.md",
                                    "summary": "OVN northd {{ $labels.instance }} has not successfully synced any changes to the southbound database for too long."
                                },
                                "expr": "# Without max_over_time, failed scrapes could create false negatives, see\n# https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.\nmax_over_time(ovnkube_controller_nb_e2e_timestamp[5m]) - max_over_time(ovnkube_controller_sb_e2e_timestamp[5m]) \u003e 120\n",
                                "for": "10m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "OVNKubernetesNorthboundDatabaseCPUUsageHigh",
                                "annotations": {
                                    "description": "High OVN northbound CPU usage indicates high load on the networking\ncontrol plane for the affected node.\n",
                                    "summary": "OVN northbound database {{ $labels.instance }} is greater than {{ $value | humanizePercentage }} percent CPU usage for a period of time."
                                },
                                "expr": "(sum(rate(container_cpu_usage_seconds_total{container=\"nbdb\"}[5m])) BY (instance, name, namespace)) \u003e 0.8",
                                "for": "15m",
                                "labels": {
                                    "severity": "info"
                                }
                            },
                            {
                                "alert": "OVNKubernetesSouthboundDatabaseCPUUsageHigh",
                                "annotations": {
                                    "description": "High OVN southbound CPU usage indicates high load on the networking\ncontrol plane for the affected node.\n",
                                    "summary": "OVN southbound database {{ $labels.instance }} is greater than {{ $value | humanizePercentage }} percent CPU usage for a period of time."
                                },
                                "expr": "(sum(rate(container_cpu_usage_seconds_total{container=\"sbdb\"}[5m])) BY (instance, name, namespace)) \u003e 0.8",
                                "for": "15m",
                                "labels": {
                                    "severity": "info"
                                }
                            },
                            {
                                "alert": "OVNKubernetesNorthdInactive",
                                "annotations": {
                                    "description": "An inactive OVN northd instance may cause a degraded networking\ncontrol plane for the affected node. Existing workloads should\ncontinue to have connectivity but new workloads may be impacted.\n",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-network-operator/OVNKubernetesNorthdInactive.md",
                                    "summary": "OVN northd {{ $labels.instance }} is not active."
                                },
                                "expr": "count(ovn_northd_status != 1) BY (instance, name, namespace) \u003e 0",
                                "for": "10m",
                                "labels": {
                                    "severity": "warning"
                                }
                            }
                        ]
                    }
                ]
            }
        },
        {
            "apiVersion": "monitoring.coreos.com/v1",
            "kind": "PrometheusRule",
            "metadata": {
                "creationTimestamp": "2025-12-02T13:30:43Z",
                "generation": 1,
                "labels": {
                    "app.kubernetes.io/managed-by": "cluster-monitoring-operator",
                    "app.kubernetes.io/part-of": "openshift-monitoring"
                },
                "name": "thanos-ruler",
                "namespace": "openshift-user-workload-monitoring",
                "resourceVersion": "11519",
                "uid": "9f155ca4-72fa-4898-9ea6-077e53cf5fc2"
            },
            "spec": {
                "groups": [
                    {
                        "name": "thanos-rule",
                        "rules": [
                            {
                                "alert": "ThanosRuleQueueIsDroppingAlerts",
                                "annotations": {
                                    "description": "Thanos Rule {{$labels.instance}} in {{$labels.namespace}} is failing to queue alerts.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/ThanosRuleQueueIsDroppingAlerts.md",
                                    "summary": "Thanos Rule is failing to queue alerts."
                                },
                                "expr": "sum by (namespace, job, instance) (rate(thanos_alert_queue_alerts_dropped_total{job=\"thanos-ruler\"}[5m])) \u003e 0\n",
                                "for": "5m",
                                "labels": {
                                    "severity": "critical"
                                }
                            },
                            {
                                "alert": "ThanosRuleSenderIsFailingAlerts",
                                "annotations": {
                                    "description": "Thanos Rule {{$labels.instance}} in {{$labels.namespace}} is failing to send alerts to alertmanager.",
                                    "summary": "Thanos Rule is failing to send alerts to alertmanager."
                                },
                                "expr": "sum by (namespace, job, instance) (rate(thanos_alert_sender_alerts_dropped_total{job=\"thanos-ruler\"}[5m])) \u003e 0\n",
                                "for": "5m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "ThanosRuleHighRuleEvaluationFailures",
                                "annotations": {
                                    "description": "Thanos Rule {{$labels.instance}} in {{$labels.namespace}} is failing to evaluate rules.",
                                    "summary": "Thanos Rule is failing to evaluate rules."
                                },
                                "expr": "(\n  sum by (namespace, job, instance) (rate(prometheus_rule_evaluation_failures_total{job=\"thanos-ruler\"}[5m]))\n/\n  sum by (namespace, job, instance) (rate(prometheus_rule_evaluations_total{job=\"thanos-ruler\"}[5m]))\n* 100 \u003e 5\n)\n",
                                "for": "5m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "ThanosRuleHighRuleEvaluationWarnings",
                                "annotations": {
                                    "description": "Thanos Rule {{$labels.instance}} in {{$labels.namespace}} has high number of evaluation warnings.",
                                    "summary": "Thanos Rule has high number of evaluation warnings."
                                },
                                "expr": "sum by (namespace, job, instance) (rate(thanos_rule_evaluation_with_warnings_total{job=\"thanos-ruler\"}[5m])) \u003e 0\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "info"
                                }
                            },
                            {
                                "alert": "ThanosRuleRuleEvaluationLatencyHigh",
                                "annotations": {
                                    "description": "Thanos Rule {{$labels.instance}} in {{$labels.namespace}} has higher evaluation latency than interval for {{$labels.rule_group}}.",
                                    "runbook_url": "https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/ThanosRuleRuleEvaluationLatencyHigh.md",
                                    "summary": "Thanos Rule has high rule evaluation latency."
                                },
                                "expr": "(\n  sum by (namespace, job, instance, rule_group) (prometheus_rule_group_last_duration_seconds{job=\"thanos-ruler\"})\n\u003e\n  sum by (namespace, job, instance, rule_group) (prometheus_rule_group_interval_seconds{job=\"thanos-ruler\"})\n)\n",
                                "for": "5m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "ThanosRuleGrpcErrorRate",
                                "annotations": {
                                    "description": "Thanos Rule {{$labels.job}} in {{$labels.namespace}} is failing to handle {{$value | humanize}}% of requests.",
                                    "summary": "Thanos Rule is failing to handle grpc requests."
                                },
                                "expr": "(\n  sum by (namespace, job, instance) (rate(grpc_server_handled_total{grpc_code=~\"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded\", job=\"thanos-ruler\"}[5m]))\n/\n  sum by (namespace, job, instance) (rate(grpc_server_started_total{job=\"thanos-ruler\"}[5m]))\n* 100 \u003e 5\n)\n",
                                "for": "5m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "ThanosRuleConfigReloadFailure",
                                "annotations": {
                                    "description": "Thanos Rule {{$labels.job}} in {{$labels.namespace}} has not been able to reload its configuration.",
                                    "summary": "Thanos Rule has not been able to reload configuration."
                                },
                                "expr": "avg by (namespace, job, instance) (thanos_rule_config_last_reload_successful{job=\"thanos-ruler\"}) != 1",
                                "for": "5m",
                                "labels": {
                                    "severity": "info"
                                }
                            },
                            {
                                "alert": "ThanosRuleQueryHighDNSFailures",
                                "annotations": {
                                    "description": "Thanos Rule {{$labels.job}} in {{$labels.namespace}} has {{$value | humanize}}% of failing DNS queries for query endpoints.",
                                    "summary": "Thanos Rule is having high number of DNS failures."
                                },
                                "expr": "(\n  sum by (namespace, job, instance) (rate(thanos_rule_query_apis_dns_failures_total{job=\"thanos-ruler\"}[5m]))\n/\n  sum by (namespace, job, instance) (rate(thanos_rule_query_apis_dns_lookups_total{job=\"thanos-ruler\"}[5m]))\n* 100 \u003e 1\n)\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "ThanosRuleAlertmanagerHighDNSFailures",
                                "annotations": {
                                    "description": "Thanos Rule {{$labels.instance}} in {{$labels.namespace}} has {{$value | humanize}}% of failing DNS queries for Alertmanager endpoints.",
                                    "summary": "Thanos Rule is having high number of DNS failures."
                                },
                                "expr": "(\n  sum by (namespace, job, instance) (rate(thanos_rule_alertmanagers_dns_failures_total{job=\"thanos-ruler\"}[5m]))\n/\n  sum by (namespace, job, instance) (rate(thanos_rule_alertmanagers_dns_lookups_total{job=\"thanos-ruler\"}[5m]))\n* 100 \u003e 1\n)\n",
                                "for": "15m",
                                "labels": {
                                    "severity": "warning"
                                }
                            },
                            {
                                "alert": "ThanosRuleNoEvaluationFor10Intervals",
                                "annotations": {
                                    "description": "Thanos Rule {{$labels.job}} in {{$labels.namespace}} has rule groups that did not evaluate for at least 10x of their expected interval.",
                                    "summary": "Thanos Rule has rule groups that did not evaluate for 10 intervals."
                                },
                                "expr": "time() -  max by (namespace, job, instance, group) (prometheus_rule_group_last_evaluation_timestamp_seconds{job=\"thanos-ruler\"})\n\u003e\n10 * max by (namespace, job, instance, group) (prometheus_rule_group_interval_seconds{job=\"thanos-ruler\"})\n",
                                "for": "5m",
                                "labels": {
                                    "severity": "info"
                                }
                            },
                            {
                                "alert": "ThanosNoRuleEvaluations",
                                "annotations": {
                                    "description": "Thanos Rule {{$labels.instance}} in {{$labels.namespace}} did not perform any rule evaluations in the past 10 minutes.",
                                    "summary": "Thanos Rule did not perform any rule evaluations."
                                },
                                "expr": "sum by (namespace, job, instance) (rate(prometheus_rule_evaluations_total{job=\"thanos-ruler\"}[5m])) \u003c= 0\n  and\nsum by (namespace, job, instance) (thanos_rule_loaded_rules{job=\"thanos-ruler\"}) \u003e 0\n",
                                "for": "5m",
                                "labels": {
                                    "severity": "warning"
                                }
                            }
                        ]
                    }
                ]
            }
        }
    ],
    "kind": "List",
    "metadata": {
        "resourceVersion": ""
    }
}
